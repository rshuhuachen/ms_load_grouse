[
  {
    "objectID": "qmd/1_test.html#quarto",
    "href": "qmd/1_test.html#quarto",
    "title": "2  main",
    "section": "2.1 Quarto",
    "text": "2.1 Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "qmd/1_test.html#running-code",
    "href": "qmd/1_test.html#running-code",
    "title": "2  main",
    "section": "2.2 Running Code",
    "text": "2.2 Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mutation Load in Black Grouse",
    "section": "",
    "text": "1 Introduction\nThis webpage contains a summary of the workflow used in the manuscript “Predicted deleterious mutations reveal the genomic mechanisms underlying fitness variation in a lekking bird”, Chen et al. 2024 (in preparation). Please see the github repository ([https://github.com/rshuhuachen/ms_load_grouse]) for all scripts and more detailed descriptions of the data and analyses.\n\n\n\nBlack grouse\n\n\n\n\n2 Main goals\nIn the current study, we used a long-term dataset to (i) quantify the fitness effects of homozygous and heterozygous individual genomic mutation loads; (ii) compare the fitness effects of deleterious mutations in coding versus noncoding regions; and (iii) unravel the behavioural and / or ornamental pathways through which deleterious mutations impact lifetime reproductive success. We used whole genome resequencing, phenotypic and fitness data of 190 male black grouse sampled annually across five study sites in Central Finland.\nMutation load can be defined as a statistic that summarizes the selection and dominance coefficients of deleterious mutations as a function of their frequencies in a population (Bertorelle et al. 2022). As we do not have selection and dominance coefficients of mutations in wild populations, we use a proxy for mutation load calculated as the number of deleterious mutations for a given individual.\nThere are different types of load, e.g. the realized load (expressed load) which reduces fitness in the current generation, and the potential/masked load (inbreeding load) which quantifies the potential fitness loss due to (partially) recessive deleterious mutations that may become expressed in future generations depending on the population’s demography. The genetic load is made up of realized plus masked load.\n\n\n3 Calculating genetic load\nThere are generally two most commonly used computational approaches to identify putative deleterious variants from whole genome re-sequencing data. In general, these tools attempt to predict the effect of a mutation on the function or evolutionary fitness of a protein. The two are distinct but can be related; for instance, a loss of function mutation will be strongly selected against if the gene is essential but will tend to be less evolutionary deleterious if the gene is non-essential or if the variant only slightly alters protein function. We used two common approaches:\n\nGenomic Evolutionary Rate Profiling (GERP): This approach uses multi-species genome alignments to identify genomic sites that are strongly conserved over millions of years of evolution, as non-synonymous mutations at these sites have a high likelihood of being deleterious. (Davydov et al. 2010)\nSNP effect (SnpEff): This approach predicts the consequences of genomic variants on protein sequences and identifies loss of function and missense variants. (Cingolani et al. 2012).\n\n\n\n4 This webpage\nOn this webpage, you will find some of the scripts for the analysis performed in this study. Note that not all bioinformatic steps are put on here (only from inferring mutations onwards). You can find the complete set of analyses with their explanations in the github repo.\n\n\n\n\nBertorelle, Giorgio, Francesca Raffini, Mirte Bosse, Chiara Bortoluzzi, Alessio Iannucci, Emiliano Trucchi, Hernán E. Morales, and Cock van Oosterhout. 2022. “Genetic Load: Genomic Estimates and Applications in Non-Model Animals.” Nature Reviews Genetics 23 (8): 492–503. https://doi.org/10.1038/s41576-022-00448-x.\n\n\nCingolani, Pablo, Adrian Platts, Le Lily Wang, Melissa Coon, Tung Nguyen, Luan Wang, Susan J. Land, Xiangyi Lu, and Douglas M. Ruden. 2012. “A Program for Annotating and Predicting the Effects of Single Nucleotide Polymorphisms, SnpEff.” Fly 6 (2): 80–92. https://doi.org/10.4161/fly.19695.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper, Arend Sidow, and Serafim Batzoglou. 2010. “Identifying a High Fraction of the Human Genome to Be Under Selective Constraint Using GERP++.” Edited by Wyeth W. Wasserman. PLoS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025."
  },
  {
    "objectID": "qmd/1_inbreeding.html#inbreeding",
    "href": "qmd/1_inbreeding.html#inbreeding",
    "title": "2  Inbreeding",
    "section": "2.1 Inbreeding",
    "text": "2.1 Inbreeding"
  },
  {
    "objectID": "qmd/1_snpeff.html#introduction",
    "href": "qmd/1_snpeff.html#introduction",
    "title": "2  SnpEff",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nSnpEff annotates genetic variants and predicts the functional effects. The output includes a VCF file with annotations that indicate what kind of mutation it is (e.g. introduction of a stop codon) and the predicted effect (low, moderate, high, modifier). In this study, focus on high impact mutations, which includes loss of function (LoF) and nonsense mediate decay (NMD) mutations."
  },
  {
    "objectID": "qmd/1_snpeff.html#methods",
    "href": "qmd/1_snpeff.html#methods",
    "title": "2  SnpEff",
    "section": "2.2 Methods",
    "text": "2.2 Methods\n\n2.2.1 Building the database\nAs the black grouse (Lyrurus tetrix) is no common model species with a pre-built database, a custom database was built from the annotation files in .gff format provided by Cantata Bio.\nTo build a custom database, five files are required: the gff file containing the gene annotation, the reference genome, and then three files containing information about the coding regions (cds.fa; a fasta file containing the coding regions only), the genes (genes.fa; a fasta file containing the genes only) and a file with the protein sequences (proteins.fa; a fasta file with the protein sequences). Two softwares were used to construct these three fasta files: gff3_to_fasta and AGAT.\ngff3_to_fasta -g data/genomic/refgenome/PO2979_Lyrurus_tetrix_black_grouse.annotation.gff \\\n    -f data/genomic/refgenome/PO2979_Lyrurus_tetrix_black_grouse.RepeatMasked.fasta -st cds -d complete -o data/genomic/refgenome/lyrurus_tetrix/cds.fa \n\ngff3_to_fasta -g data/genomic/refgenome/PO2979_Lyrurus_tetrix_black_grouse.annotation.gff \\\n    -f data/genomic/refgenome/PO2979_Lyrurus_tetrix_black_grouse.RepeatMasked.fasta -st gene -d complete -o data/genomic/refgenome/lyrurus_tetrix/genes.fa \nSimilarly, the protein sequences were constructed with AGAT\nagat_sp_extract_sequences.pl --gff data/genomic/refgenome/PO2979_Lyrurus_tetrix_black_grouse.annotation.gff -f \\\n    data/genomic/refgenome/PO2979_Lyrurus_tetrix_black_grouse.RepeatMasked.fasta -p -o \\\n    data/genomic/refgenome/lyrurus_tetrix/protein.fa\nThen, the database was built (and automatically checked).\njava -jar snpEff.jar build -gff3 -v data/genomic/refgenome/lyrurus_tetrix\nOnce the database is ready, we can run SnpEff to create the annotated vcf file.\njava -Xmx8g -jar snpEff.jar ann -stats  \\\n-no-downstream -no-intergenic -no-intron -no-upstream -no-utr -v \\\nlyrurus_tetrix data/genomic/intermediate/ltet_snps_filtered.vcf &gt; data/genomic/intermediate/snpef/ltet_ann_snp_output.vcf\n\n\n2.2.2 Ancestral alleles\nSnpEff annotates mutations according to the change from the reference allele to the focal allele. Hence, it assumes that the reference allele is the ‘better’ one and that a mutation that changes the transcription of this reference allele is detrimental. To allow this assumption to be better met, we used the ancestral genome as a reference, instead of the reference genome itself (i.e. we polarized the genome). This ancestral genome is constructed by cactus, and represents the most recent common ancestor between black grouse (L. tetrix) and Lagoplus leucura (white tailed ptarmigan). This way, any derived allele was assumed to be ‘deleterious’ compared to the ancestral allele, as opposed to a reference-non reference comparison.\n\n\n2.2.3 Filtering\nWe then used SnpSift to filter annotated mutations based on the four impact categories: modifier, low, moderate and high impact using the following commands.\n\n## High impact\nzcat output/ancestral/ltet_filtered_ann_aa.vcf.gz | java -jar src/SnpSift.jar filter \" ( ANN[*].IMPACT = 'HIGH' )\" &gt; data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_HIGH.vcf\ngzip data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_HIGH.vcf\n\n## Moderate\nzcat output/ancestral/ltet_filtered_ann_aa.vcf.gz | java -jar src/SnpSift.jar filter \" ( ANN[*].IMPACT = 'MODERATE')\" &gt; data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_moderate.vcf\ngzip data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_moderate.vcf\n\n## Low\nzcat output/ancestral/ltet_filtered_ann_aa.vcf.gz | java -jar src/SnpSift.jar filter \" ( ANN[*].IMPACT = 'LOW') \" &gt; data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_low.vcf\ngzip data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_low.vcf"
  },
  {
    "objectID": "qmd/references.html",
    "href": "qmd/references.html",
    "title": "References",
    "section": "",
    "text": "Bertorelle, Giorgio, Francesca Raffini, Mirte Bosse, Chiara Bortoluzzi,\nAlessio Iannucci, Emiliano Trucchi, Hernán E. Morales, and Cock van\nOosterhout. 2022. “Genetic Load: Genomic Estimates and\nApplications in Non-Model Animals.” Nature Reviews\nGenetics 23 (8): 492–503. https://doi.org/10.1038/s41576-022-00448-x.\n\n\nCingolani, Pablo, Adrian Platts, Le Lily Wang, Melissa Coon, Tung\nNguyen, Luan Wang, Susan J. Land, Xiangyi Lu, and Douglas M. Ruden.\n2012. “A Program for Annotating and Predicting the Effects of\nSingle Nucleotide Polymorphisms, SnpEff.”\nFly 6 (2): 80–92. https://doi.org/10.4161/fly.19695.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper,\nArend Sidow, and Serafim Batzoglou. 2010. “Identifying a\nHigh Fraction of the Human Genome to Be Under\nSelective Constraint Using GERP++.” Edited by Wyeth\nW. Wasserman. PLoS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025."
  },
  {
    "objectID": "qmd/1_snpeff.html#results",
    "href": "qmd/1_snpeff.html#results",
    "title": "2  SnpEff",
    "section": "2.3 Results",
    "text": "2.3 Results\nWe identified 5,341 high impact mutations:\n\n\n\nSnpEff annotation\n\n\nExisting of mostly LoF mutations and gained stop codons (non-mutually exclusive)\n\n\n\nDetailed SnpEff annotation\n\n\nThe mutations in the ‘high impact’ category were used to calculate individual genomic mutation load estimates."
  },
  {
    "objectID": "qmd/2_gerp.html#introduction",
    "href": "qmd/2_gerp.html#introduction",
    "title": "3  GERP",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nGERP++ annotates a focal genome based on evolutionary conserveration, where regions in the genome that show higher conservation across multiple different species are expected to face higher selective constraint. The calculation of GERP scores, which indicate the reduction in the number of substitutions compared to neutral expectations, is done from a multi-species alignment file. Higher GERP scores indicate higher evolutionary constraint. First, we create this MAF file using Progressive Cactus, then we calculate GERP scores genome-wide, and select genomic positions with SNPs in our population."
  },
  {
    "objectID": "qmd/2_gerp.html#methods",
    "href": "qmd/2_gerp.html#methods",
    "title": "3  GERP",
    "section": "3.2 Methods",
    "text": "3.2 Methods\n\n3.2.1 Creating the MAF\nWe use the publicly available 363 avian genomes multi-alignment file as a starting point, and then reduce this file to exclude species of the Neoaves clade. All the GERP analyses were done using the cactus container.\n\n\nCode\n### Remove subtrees that are not needed for ltet analysis\n\n#set cactus scratch directory\nCACTUS_SCRATCH=$(pwd)/scratch/\n\n# enter the container\napptainer shell --cleanenv \\\n  --fakeroot --overlay ${CACTUS_SCRATCH} \\\n  --bind ${CACTUS_SCRATCH}/tmp:/tmp,$(pwd) \\\n  --env PYTHONNOUSERSITE=1 \\\n  docker:quay.io/comparative-genomics-toolkit/cactus:v2.5.1 \n\n# get stats on the original 363-avian multi-alignment file\nhalStats data/genomic/intermediate/cactus/363-avian-2020.hal &gt; output/cactus/stats_original_363_hal.txt\n\n# copy the original file to then edit it\ncp data/genomic/intermediate/cactus/363-avian-2020.hal data/genomic/intermediate/cactus/363-avian-reduced.hal\n\n# remove subtrees to exclude neoaves\nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc1\nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc57 \nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc69 \nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc318 #starts with Heliornis_fulica\nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc319 #starts with Psophia_crepitans\nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc320 #starts with Charadrius_vociferus\nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc321 #starts with Opisthocomus_hoazin\nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc322 #stars with birdAnc57, so the big chunk of passerines but also a \n\n#some individual ancestral genomes left to exclude\nhalRemoveGenome data/genomic/intermediate/cactus//363-avian-reduced.hal birdAnc322\nhalRemoveGenome data/genomic/intermediate/cactus//363-avian-reduced.hal birdAnc1\n\n# get stats of our subset of genomes\n\nhalStats data/genomic/intermediate/cactus/363-avian-reduced.hal &gt; output/cactus/stats_reduced_363_hal.txt\n\n#extract the reduced file \nhalExtract data/genomic/intermediate/cactus/363-avian-reduced.hal data/genomic/intermediate/cactus/363-avian-reduced.hal \n\n\nNext, we add two genomes: Lyrurus tetrix and Lagopus lecura, using the cactus prepare function (2_cactus_prepare.txt). To add the Lagopus lecura genome to the hal file, the assembly needs to be downloaded from NCBI which can be found on NCBI: https://0-www-ncbi-nlm-nih-gov.brum.beds.ac.uk/datasets/genome/GCF_019238085.1/.\n\n\nCode\n# In this file, we will use the cactus-update-prepare function to create two scripts that will allow us to add two genomes to our dataset\n\n# set scratch directory\nCACTUS_SCRATCH=$(pwd)/scratch/\n\n#enter container\napptainer shell --cleanenv \\\n  --fakeroot --overlay ${CACTUS_SCRATCH} \\\n  --bind ${CACTUS_SCRATCH}/tmp:/tmp,$(pwd) \\\n  --env PYTHONNOUSERSITE=1 \\\n  src/containers/cactus_v2.5.1.sif \n\nsh\n\n#first add Lyrurus tetrix, branchlengths will be corrected in a later step\n\ncactus-update-prepare \\\n  add branch \\\n  --parentGenome birdAnc334 \\\n  --childGenome Tympanuchus_cupido \\\n  data/genomic/intermediate/cactus/363-avian-reduced.hal \\\n  scripts/2_cactus/input_ltet.txt \\\n  --cactus-prepare-options \\\n  '--alignCores 4' \\\n  --topBranchLength 0.01 \\ \n  --outDir scratch/tmp/steps-output \\\n  --jobStore scratch/tmp/js \\\n  --ancestorName AncX &gt; scripts/2_cactus/3_cactus_update_lyrurus_steps.sh \n\n# then add Lagopus leucura\n\ncactus-update-prepare \\\n  add branch \\\n  --parentGenome AncX \\\n  --childGenome Lyrurus_tetrix \\\n  data/genomic/intermediate/cactus/363-avian-reduced.hal \\\n  scripts/2_cactus/input_lleu.txt \\\n  --cactus-prepare-options \\\n  '--alignCores 4' \\\n  --topBranchLength 0.01 \\\n  --outDir scratch/tmp/steps-output \\\n  --jobStore scratch/tmp/js \\\n  --ancestorName AncY &gt; scripts/2_cactus/4_cactus_update_lagopus_steps.sh \n  \n\n\nThis is just the preparation step, and two files will be outputted that can be used to update the .hal file. This is what these files look like (and then they have to be executed).\n\n\nCode\n## Preprocessor\ncactus-preprocess scratch/tmp/js/0 scratch/tmp/steps-output/seq_file.in scratch/tmp/steps-output/seq_file.out --inputNames Lyrurus_tetrix --realTimeLogging --logInfo --retryCount 0 --maskMode none\n\n## Alignment\n\n### Round 0\ncactus-blast scratch/tmp/js/1 scratch/tmp/steps-output/seq_file.out scratch/tmp/steps-output/AncX.cigar --root AncX --restart \ncactus-align scratch/tmp/js/2 scratch/tmp/steps-output/seq_file.out scratch/tmp/steps-output/AncX.cigar scratch/tmp/steps-output/AncX.hal --root AncX  --maxCores 8 \nhal2fasta scratch/tmp/steps-output/AncX.hal AncX --hdf5InMemory &gt; scratch/tmp/steps-output/AncX.fa \n\n### Round 1\ncactus-blast scratch/tmp/js/3 scratch/tmp/steps-output/seq_file.out scratch/tmp/steps-output/birdAnc334.cigar --root birdAnc334 --includeRoot  \ncactus-align scratch/tmp/js/4 scratch/tmp/steps-output/seq_file.out scratch/tmp/steps-output/birdAnc334.cigar scratch/tmp/steps-output/birdAnc334.hal --root birdAnc334  --maxCores 4 --includeRoot  \n\n## Alignment update\nhalAddToBranch data/genomic/intermediate/cactus/363-avian-reduced.hal scratch/tmp/steps-output/AncX.hal scratch/tmp/steps-output/birdAnc334.hal birdAnc334 AncX Tympanuchus_cupido Lyrurus_tetrix 0.01 1.0 --hdf5InMemory \n\n## Alignment validation\nhalValidate --genome birdAnc334 data/genomic/intermediate/cactus/363-avian-reduced.hal --hdf5InMemory\nhalValidate --genome AncX data/genomic/intermediate/cactus/363-avian-reduced.hal --hdf5InMemory\nhalValidate --genome Tympanuchus_cupido data/genomic/intermediate/cactus/363-avian-reduced.hal --hdf5InMemory\nhalValidate --genome Lyrurus_tetrix data/genomic/intermediate/cactus/363-avian-reduced.hal --hdf5InMemory\n\n\nFor subsequent steps, the resulting hal file is converted to maf format per scaffold for both GERP++ and the neutral tree calculation. Note that we only focus on the 30 largest scaffolds of the black grouse genome which over &gt;95% of the genome, and only autosomal scaffolds.\nThis conversion is done within an R script\n\n\nCode\nhal =  \"data/genomic/intermediate/cactus/363-avian-reduced.hal\"\nlibrary(dplyr); library(data.table)\nscafs &lt;- fread(\"data/genomic/refgenome/30_largest.scafs.tsv\")\noutput_dir = \"output/cactus/maf_per_scaf\"\nsif = \"src/containers/cactus_v2.6.12.sif\"\nscratch = \"scripts/2_cactus/scratch\"\ntmp_js = \"scripts/2_cactus/scratch/tmp/js/wiggle\"\n\nhal_to_maf_per_scaf &lt;- function(hal, scaf, outdir, scratch, i, sif, tmp_js){\n    system(paste0('/vol/apptainer/bin/apptainer run --cleanenv --fakeroot --overlay ', scratch, ' --bind ', scratch, '/tmp:/tmp,', scratch, ' --env PYTHONNOUSERSITE=1 ', sif, ' cactus-hal2maf ', tmp_js, '/js_', i, ' --restart ', hal, ' ', outdir, '/maf_', i, '.maf --refGenome Lyrurus_tetrix --refSequence ', scaf, ' --dupeMode single --filterGapCausingDupes --chunkSize 1000000 --noAncestors'))\n}\n\nfor (i in 1:30){\n  hal_to_maf_per_scaf(hal = hal,\n  scaf = scafs$scaf[i],\n  outdir = output_dir,\n  scratch = scratch,\n  sif = sif,\n  i = i,\n  tmp_js = tmp_js)\n}\n\n\nLastly, the final phylogenetic tree has to be recalculated according to the updated tree, and the branch lengths have to be calculated in substitutions/site (rather than million years ago). This analysis can be found on github and will not be included here as it contains many small steps integrated with in a snakemake workflow.\n\n\n3.2.2 Calculate GERP scores\nGERP scores were calculated per scaffold using snakemake using the following rule:\n\n\nCode\nrule call_gerp:\n    input:\n      maf = \"output/cactus/maf_per_scaf/maf_{scaf}.maf\",\n      tree = \"output/tree_cactus_updated.txt\"\n    output:\n      rates = \"output/gerp/maf_{scaf}.maf.rates\"\n    params:\n      refname = \"Lyrurus_tetrix\"\n    log: \"logs/gerp_{scaf}.log\"\n    shell:\n      \"\"\"\n      gerpcol -t {input.tree} -f {input.maf} -e {params.refname} -j -z -x \".rates\" &&gt; {log}\n      \"\"\"\n\n\n\n\n3.2.3 Overlap GERP scores with SNPs\nWe are only interested in genomic locations where SNPs were found in our population. Therefore, we convert our VCF file and GERP files to bed format to overlap the SNPs with bedtools using the following commands (integrated in snakemake):\n\n\nCode\nrule vcf_to_bed:\n    input:\n        vcf = \"output/ancestral/ltet_filtered_ann_aa.vcf.gz\"\n    output:\n        bed = \"output/ancestral/ltet_filtered_ann_aa.bed\"\n    shell:\n        \"\"\"\n        convert2bed -i vcf &lt; {input.vcf} &gt; {output.bed}\n\nrule gerp_to_bed:\n    input:\n        rates = \"output/gerp/maf_per_scaf/biggest_30/maf_{nscaf}.maf.rates\"\n    output:\n        bed = \"output/gerp/beds/gerp_scaf_{nscaf}.bed\"\n    params:\n        outdir = \"output/gerp/beds\"\n    log: \"logs/gerp_to_bed_{nscaf}\"\n    shell:\n        \"\"\"\n        Rscript --vanilla scripts/6_snpeff_gerp/2_gerp/gerp_to_bed.R {input.rates} {output.bed} {params.outdir} &&gt; {log}\n        \"\"\"\n\nrule bed_overlap_snps:\n    input:\n        bed = \"output/gerp/beds/gerp_scaf_{nscaf}.bed\",\n        snps = \"output/ancestral/ltet_filtered_ann_aa.bed\"\n    output:\n        tsv = \"output/gerp/beds/gerp_overlapSNP_scaf_{nscaf}.tsv.gz\"\n    params:\n        tsv = \"output/gerp/beds/gerp_overlapSNP_scaf_{nscaf}.tsv\"    \n    shell:\n        \"\"\"\n        bedtools intersect \\\n        -a {input.bed} \\\n        -b {input.snps} \\\n        -wa -wb |\n        cut -f 6-10 --complement &gt; {params.tsv}\n\n        gzip {params.tsv}\n        \"\"\"\n\n\nAs these files are still very large, we loop over scaffolds within snakemake with an R script to count the number of mutations per individual per scaffold, both in homozygosity and heterozygosity using the following R formula (which is used for each scaffold separately and outputs a tsv file used for calculating mutation load in the next script).\n\n\nCode\ncalculate_gerp_load &lt;- function(gerp_vcf, scafno){\n  ## metadata on filenames and ids\n  filenames &lt;- fread(\"data/genomic/raw/metadata/idnames.fam\")\n  ids &lt;- fread(\"data/genomic/raw/metadata/file_list_all_bgi_clean.csv\")\n  \n  #merge\n  idnames &lt;- left_join(filenames[,c(\"V1\")], ids[,c(\"loc\", \"id\")], by = c(\"V1\" = \"loc\"))\n  \n  file &lt;- read_tsv(gerp_vcf, col_names = c(\"chr\", \"start\", \"pos\", \"neutral_rate_n\", \"rs_score\", \"ancestral\", \"derived\", \"qual\", \"info\",\"format\", idnames$id) )#rename columns\n  \n  # only get GT info, PL and DP are filtered by already anyway \n  gt &lt;- c(11:ncol(file))\n  select_n3 &lt;- function(x){x = substr(x,1,3)}\n  file[gt] &lt;- lapply(file[gt], select_n3)\n  \n  # replace genotype with RS value but separate per zygosity, do per ID\n  gerp_load &lt;- list()\n  for( id in 11:ncol(file)){\n    subset_id &lt;- file[,c(1:10, id)]\n    subset_id &lt;- subset_id %&gt;% mutate(gerp_cat = as.factor(case_when(\n        rs_score &lt; 0 ~ \"&lt; 0\", #changed\n        rs_score &gt;= 0 & rs_score &lt; 1 ~ \"0-1\",\n        rs_score &gt;= 1 & rs_score &lt; 2 ~ \"1-2\",\n        rs_score &gt;= 2 & rs_score &lt; 3 ~ \"2-3\",\n        rs_score &gt;= 3 & rs_score &lt; 4 ~ \"3-4\",\n        rs_score &gt;= 4 ~ \"4-5\"\n    )))\n    gerp_load_id &lt;- list()\n    for (i in c(\"&lt; 0\", \"0-1\", \"1-2\", \"2-3\", \"3-4\", \"4-5\")){#changed\n        cat_subset &lt;- subset(subset_id, gerp_cat == i)\n        het_data &lt;- subset(cat_subset, cat_subset[[11]] == \"1/0\" | cat_subset[[11]] == \"0/1\")\n        hom_data &lt;- subset(cat_subset, cat_subset[[11]] == \"1/1\")\n        n_genotyped &lt;- nrow(cat_subset) - nrow(subset(cat_subset, cat_subset[[11]] == \"./.\"))\n        n_total &lt;- nrow(cat_subset)\n        df &lt;- data.frame(id = colnames(file[id]),\n                         gerp_cat = i,\n                         scafno = scafno,\n                         n_total = n_total,\n                         n_genotyped = n_genotyped,\n                         het_data = nrow(het_data),\n                         hom_data = nrow(hom_data))\n        \n        gerp_load_id[[i]] &lt;- df\n        \n        }\n        gerp_load_id &lt;- do.call(rbind.data.frame, gerp_load_id)\n        rownames(gerp_load_id) &lt;- NULL\n    \n    gerp_load[[id]] &lt;- gerp_load_id\n    }\n  gerp_load &lt;- do.call(rbind.data.frame, gerp_load)\n\n    return(gerp_load)}"
  },
  {
    "objectID": "qmd/3_load.html#results",
    "href": "qmd/3_load.html#results",
    "title": "4  Calculating mutation load",
    "section": "4.1 Results",
    "text": "4.1 Results\n\n4.1.1 Number of mutations per impact class\nFirst, let’s look at the distribution of the number of mutations per type and impact class that we identified in this population (Figure 4.1).\n\n\n\nFigure 4.1: Histograms mutations per type\n\n\nHere we can see that every LoF mutation is a high impact mutation.\n\n\n4.1.2 Number of mutations per scaffold\nA total of 261 scaffolds (out of the total 21,979 scaffolds) contain a mutation (either missense, high impact or LOF). You can see the distribution of the number of mutations on a scaffold below (Figure 4.2):\n\n\n\nFigure 4.2: Histogram of the number of mutations on a scaffold\n\n\n\n\n4.1.3 Distribution of mutations\nI used a sliding window approach to visualise the distribution of mutations across the genome. I only took the 25 largest scaffolds with mutations, which still includes 95% of mutations. The sliding windows were 500kb in size and I summed the number of mutations present in the window for each for each of the mutation types separately. See below the distribution for high impact mutations (Figure 4.3):\n\n4.1.3.1 Distribution high impact mutations\n\n\n\nFigure 4.3: Distribution of high impact mutations across the genome\n\n\nIt’s interesting that the patterns are very similar across the mutation types, that there are peaks at the edges of the scaffolds which is in line with recombination rates being higher at the ends of chromosomes. There are higher peaks in larger scaffolds, and in general more mutations in larger scaffolds compared to smaller ones.\n\n\n\n4.1.4 Relationship number of mutations and scaffold size\nOne hypothesis to explain high realised load of lekking males is through genetic hitchhiking of deleterious mutations alongside positively selected sexual traits (QTLs). (Figure 4.4). Over the course of evolution, selection will favour sexually selected QTLs to move on micro-chromosomes to reduce the impact of this genetic hitchhiking. Although we do not have chromosome-level assembled scaffolds, we can still investigate the relationship between scaffold size and the number of mutations.\n\n\n\nFigure 4.4: Relationship scaffold size and the number of mutations\n\n\nIt indeed seems like the smaller scaffolds have higher number of mutations, in all mutation classes. This result should be interpreted with care though, as scaffold size is not necessarily representative of chromosome size.\n\n\n4.1.5 Genetic load\nThen, based on the definitions described above, I calculated realized, potential and total genetic load on an individual basis. We can see the distribution of the different load types below(Figure 4.5).\n\n\n\nFigure 4.5: Histogram load per type per ID"
  },
  {
    "objectID": "qmd/2_gerp.html#results",
    "href": "qmd/2_gerp.html#results",
    "title": "3  GERP",
    "section": "3.3 Results",
    "text": "3.3 Results\nWe identified 413,489 mutations with a GERP score higher than or equal to 4: \nThese mutations were used to calculate mutation load in the next script."
  },
  {
    "objectID": "qmd/3_load.html#introduction",
    "href": "qmd/3_load.html#introduction",
    "title": "4  Calculating mutation load",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThere are various types of load and in general, mutation load can be divided between potential and realized load. Realized load (also known as expressed load) only includes the deleterious mutations that are expressed in the individual (Bertorelle et al., 2022). Potential load (also known as inbreeding or masked load) is the fitness reduction due to deleterious mutations, of which not all are expressed on an individual level and therefore quantifies recessive deleterious mutations that could be expressed in future generations (Bertorelle et al., 2022).\nHowever, to be able to distinguish between realized and potential load, you need to know dominance coefficients, which we do not. Therefore, we focus on homozygous and heterozygous load instead, which consists of mutations in homo- and heterozygosity in an individual instead. The total load sums the number of mutations contributing to the load, where heterozygous mutations are counted ones (one per allele) and homozygous mutations twice (one per allele).\nFrom here on onwards, the majority of analyses are computed within R (instead of bash scripts/snakemake)."
  },
  {
    "objectID": "qmd/3_load.html#mutation-load-snpeff",
    "href": "qmd/3_load.html#mutation-load-snpeff",
    "title": "4  Calculating mutation load",
    "section": "4.2 Mutation load (SnpEff)",
    "text": "4.2 Mutation load (SnpEff)\nHere, we load in the .vcf file outputted by SnpSift with only high impact SnpEff mutations, add column names, include only the 29 largest autosomal scaffolds, exclude warning messages, convert the genotype columns into only 1/1, 1/0, 0/1, 0/0 and ./. values, calculate load per individual, and then merge the load estimates of all individuals together.\n\n\nCode\n### load packages ###\npacman::p_load(dplyr, data.table)\n\n### function to calculate load ###\ncalculate_load_snpeff &lt;- function(vcf, output_vcf, loadtype){\n  ## metadata on filenames and ids\n  filenames &lt;- fread(\"data/genomic/raw/metadata/idnames.fam\")\n  ids &lt;- fread(\"data/genomic/raw/metadata/file_list_all_bgi_clean.csv\")\n  \n  #merge\n  idnames &lt;- left_join(filenames[,c(\"V1\")], ids[,c(\"loc\", \"id\")], by = c(\"V1\" = \"loc\"))\n  \n  names(vcf) &lt;- c(c(\"CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\", \"QUAL\", \"FILTER\", \"INFO\",\"FORMAT\"), idnames$id)#rename columns\n\n  # only select 29 largest autosomal scaffolds\n  scaf &lt;- fread(\"data/genomic/refgenome/30_largest.scafs.tsv\")\n  scaf$scaf &lt;- gsub(\":\", \";\", scaf$scaf)\n  scaf$scaf &lt;- gsub(\"\\\\.\", \"=\", scaf$scaf)\n  scaf &lt;- subset(scaf, scaf_no != 4)\n  \n  vcf &lt;- subset(vcf, CHROM %in% scaf$scaf)\n  \n  # exclude warning messages\n  \n  vcf &lt;- subset(vcf, !grepl(\"WARNING\", INFO))\n  \n  # only get GT info, PL and DP are filtered by already anyway \n  gt &lt;- c(10:ncol(vcf))\n  select_n3 &lt;- function(x){x = substr(x,1,3)}\n  vcf[gt] &lt;- lapply(vcf[gt], select_n3)\n  \n  # calculate load\n  load &lt;- list()\n  # loop over ids\n  for( id in 10:(ncol(vcf))){\n    # subset per id\n    subset_id &lt;- vcf[,c(1:9, id)]\n    \n    # filter for snps in het and hom\n    het_data &lt;- subset(subset_id, subset_id[[10]] == \"1/0\" | subset_id[[10]] == \"0/1\")\n    hom_data &lt;- subset(subset_id, subset_id[[10]] == \"1/1\")\n    \n    # count amount of snps in het and hom\n    het_load_sum &lt;- nrow(het_data)\n    hom_load_sum &lt;- nrow(hom_data)\n    \n    # count no of snps successfully genotyped\n    n_genotyped &lt;- nrow(subset_id) - nrow(subset(subset_id, subset_id[[10]] == \"./.\"))\n    n_total &lt;- nrow(subset_id)\n    \n    # collect data in df\n    df &lt;- data.frame(id = colnames(vcf[id]),\n                     n_total = n_total,\n                     n_genotyped = n_genotyped,\n                     het_load = het_load_sum / n_genotyped,\n                     hom_load = hom_load_sum / n_genotyped,\n                     total_load = (het_load_sum*0.5 + hom_load_sum) / n_genotyped,\n                     loadtype = loadtype)\n    load[[id]] &lt;- df\n  }\n  # convert list to df\n  load &lt;- do.call(rbind.data.frame, load)\n  \n  if(output_vcf == TRUE){\n    out &lt;- list(load = load, vcf = vcf)\n    return(out)\n  }\n  \n  if(output_vcf==FALSE){\n    return(load)}\n}\n\n##### load high impact mutations (filtered by snpsift) #####\n\nhigh &lt;- read.table(\"data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_HIGH.vcf.gz\")\n\n## calculate load \n# in this function, we give the columns names, filter for only the largest 29 autosomal scaffolds and exclude annotations with warning messages\n\nhigh_load &lt;- calculate_load_snpeff(high, output_vcf = TRUE, loadtype = \"high\")"
  },
  {
    "objectID": "qmd/3_load.html#mutation-load-gerp",
    "href": "qmd/3_load.html#mutation-load-gerp",
    "title": "4  Calculating mutation load",
    "section": "4.3 Mutation load (GERP)",
    "text": "4.3 Mutation load (GERP)\nHere, we load in the .bed files that contain GERP scores from SNPs, filter for those with GERP values &gt;= 4, add column names, convert the genotype columns into only 1/1, 1/0, 0/1, 0/0 and ./. values, calculate load per individual, and then merge the load estimates of all individuals together. Note this step is quite time-intensive as the .bed files are large in filesize!\n\n\nCode\n# load in all bed files with gerp scores that overlap a SNP\ngerp_snp_scafs &lt;- list.files(path = \"output/gerp/beds\", pattern = \"gerp_overlapSNP*\", full.names = T)\ngerp_snp_scafs &lt;- gerp_snp_scafs[-22] #empty, scaffold 29 has no SNPs with gerp scores\n\ngerp_snp &lt;- data.frame()\nfor (i in 1:length(gerp_snp_scafs)){\n  scaf &lt;- read.table(gerp_snp_scafs[i])\n  scaf &lt;- scaf %&gt;% filter(V5 &gt;= 4)\n  gerp_snp &lt;- rbind(gerp_snp, scaf)\n}\n\n## function to calculate load\n\ncalculate_load_gerp &lt;- function(vcf, output_vcf, loadtype){\n  \n  ## metadata on filenames and ids\n  filenames &lt;- fread(\"data/genomic/raw/metadata/idnames.fam\")\n  ids &lt;- fread(\"data/genomic/raw/metadata/file_list_all_bgi_clean.csv\")\n  \n  #merge\n  idnames &lt;- left_join(filenames[,c(\"V1\")], ids[,c(\"loc\", \"id\")], by = c(\"V1\" = \"loc\"))\n  \n  names(vcf) &lt;- c(\"chr\", \"start\", \"pos\", \"neutral_rate_n\", \"rs_score\", \"ancestral\", \"derived\", \"qual\", \"info\",\"format\", idnames$id) #rename columns\n  \n  # only get GT info, PL and DP are filtered by already anyway \n  gt &lt;- c(11:ncol(vcf))\n  select_n3 &lt;- function(x){x = substr(x,1,3)}\n  vcf[gt] &lt;- lapply(vcf[gt], select_n3)\n  \n  # calculate load\n  load &lt;- list()\n  # loop over ids\n  for( id in 11:(ncol(vcf))){\n    # subset per id\n    subset_id &lt;- vcf[,c(1:10, id)]\n    \n    # filter for snps in het and hom\n    het_data &lt;- subset(subset_id, subset_id[[11]] == \"1/0\" | subset_id[[11]] == \"0/1\")\n    hom_data &lt;- subset(subset_id, subset_id[[11]] == \"1/1\")\n    \n    # count amount of snps in het and hom\n    het_load_sum &lt;- nrow(het_data)\n    hom_load_sum &lt;- nrow(hom_data)\n    \n    # count no of snps successfully genotyped\n    n_genotyped &lt;- nrow(subset_id) - nrow(subset(subset_id, subset_id[[11]] == \"./.\"))\n    n_total &lt;- nrow(subset_id)\n    \n    # collect data in df\n    df &lt;- data.frame(id = colnames(vcf[id]),\n                     n_total = n_total,\n                     n_genotyped = n_genotyped,\n                     het_load = het_load_sum / n_genotyped,\n                     hom_load = hom_load_sum / n_genotyped,\n                     total_load = (het_load_sum*0.5 + hom_load_sum) / n_genotyped,\n                     loadtype = loadtype)\n    load[[id]] &lt;- df\n  }\n  # convert list to df\n  load &lt;- do.call(rbind.data.frame, load)\n  \n  if(output_vcf == TRUE){\n    out &lt;- list(load = load, vcf = vcf)\n    return(out)\n  }\n  \n  if(output_vcf==FALSE){\n  return(load)}\n}\n\n## calculate load\ngerp_45 &lt;- calculate_load_gerp(gerp_snp, output_vcf = TRUE, loadtype = \"gerp45\") \ngerp &lt;- gerp_45_load_check$vcf\n\n\n\n4.3.1 Combine loads\nNote: the analyses done above was also done for other mutation categories, e.g. low and moderate impact classes and GERP scores between 3-4. All load scores are then combined into a single file:\n\n\nCode\nload &lt;- rbind(high_load$load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")] ,\n              moderate_load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")], \n              low_load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")],\n              lof_load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")],\n              missense_load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")], \n              gerp_34_load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")], \n              gerp_45_load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")])\n\nsave(load, file = \"output/load/all_loads_combined_da_nosex_29scaf.RData\")\nwrite.table(load, file = \"output/load/all_loads_combined_da_nosex_29scaf.tsv\", sep=\"\\t\", row.names = F)\n\n\nWe can then calculate the correlation between the two load estimates and test for lek effects on load.\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nload(file = \"../output/load/all_loads_combined_da_nosex_29scaf.RData\")\n\ncor.test(load$total_load[which(load$loadtype == \"gerp45\")], load$total_load[which(load$loadtype == \"high\")])\n\n\n\n    Pearson's product-moment correlation\n\ndata:  load$total_load[which(load$loadtype == \"gerp45\")] and load$total_load[which(load$loadtype == \"high\")]\nt = 1.7867, df = 188, p-value = 0.07559\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.01338135  0.26666622\nsample estimates:\n      cor \n0.1292181 \n\n\nCode\n### Test for lek effects ####\nload(\"../data/phenotypes/phenotypes_lifetime.RData\")\npheno_load &lt;- left_join(pheno_wide, load, by = \"id\")\n\nsummary(lm(total_load ~ site, data = subset(pheno_load, loadtype == \"gerp45\")))\n\n\n\nCall:\nlm(formula = total_load ~ site, data = subset(pheno_load, loadtype == \n    \"gerp45\"))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.426e-03 -2.162e-04  4.882e-05  3.303e-04  1.308e-03 \n\nCoefficients:\n              Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)  1.519e-01  6.684e-05 2272.733   &lt;2e-16 ***\nsiteLEH     -8.574e-05  1.171e-04   -0.732    0.465    \nsiteNYR     -5.353e-05  9.711e-05   -0.551    0.582    \nsiteSAA     -5.700e-05  1.200e-04   -0.475    0.635    \nsiteTEE      4.854e-05  1.337e-04    0.363    0.717    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0005177 on 185 degrees of freedom\nMultiple R-squared:  0.006348,  Adjusted R-squared:  -0.01514 \nF-statistic: 0.2955 on 4 and 185 DF,  p-value: 0.8807\n\n\nCode\nsummary(lm(total_load ~ site, data = subset(pheno_load, loadtype == \"high\")))\n\n\n\nCall:\nlm(formula = total_load ~ site, data = subset(pheno_load, loadtype == \n    \"high\"))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0098650 -0.0020586  0.0003317  0.0020222  0.0083354 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.1653480  0.0003800 435.179   &lt;2e-16 ***\nsiteLEH     -0.0001091  0.0006656  -0.164    0.870    \nsiteNYR     -0.0001785  0.0005521  -0.323    0.747    \nsiteSAA     -0.0009170  0.0006820  -1.344    0.180    \nsiteTEE     -0.0005430  0.0007599  -0.715    0.476    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.002943 on 185 degrees of freedom\nMultiple R-squared:  0.01131,   Adjusted R-squared:  -0.01007 \nF-statistic: 0.5289 on 4 and 185 DF,  p-value: 0.7146"
  }
]