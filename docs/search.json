[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mutation Load in Black Grouse",
    "section": "",
    "text": "1 Introduction\nThis webpage/document contains a summary of the workflow used in the manuscript titled “Predicted deleterious mutations reveal the genomic mechanisms underlying fitness variation in a lekking bird”, Chen et al. 2024 (in preparation). Please see the github repository for all scripts and more detailed descriptions of the data and analyses.\n\n\n\nBlack grouse\n\n\n\n\n2 Main goals\nIn the current study, we used a long-term dataset to (i) quantify the fitness effects of homozygous and heterozygous individual genomic mutation loads; (ii) compare the fitness effects of deleterious mutations in coding versus noncoding regions; and (iii) unravel the behavioural and / or ornamental pathways through which deleterious mutations impact lifetime reproductive success. We used whole genome resequencing, phenotypic and fitness data of 190 male black grouse sampled annually across five study sites in Central Finland.\nMutation load can be defined as a statistic that summarizes the selection and dominance coefficients of deleterious mutations as a function of their frequencies in a population (Bertorelle et al. 2022). As we do not have selection and dominance coefficients of mutations in wild populations, we use a proxy for mutation load calculated as the number of deleterious mutations for a given individual.\nThere are different types of load, e.g. the realized load (expressed load) which reduces fitness in the current generation, and the potential/masked load (inbreeding load) which quantifies the potential fitness loss due to (partially) recessive deleterious mutations that may become expressed in future generations depending on the population’s demography. The genetic load is made up of realized plus masked load.\n\n\n3 Calculating genetic load\nThere are generally two most commonly used computational approaches to identify putative deleterious variants from whole genome re-sequencing data. In general, these tools attempt to predict the effect of a mutation on the function or evolutionary fitness of a protein. The two are distinct but can be related; for instance, a loss of function mutation will be strongly selected against if the gene is essential but will tend to be less evolutionary deleterious if the gene is non-essential or if the variant only slightly alters protein function. We used two common approaches:\n\nGenomic Evolutionary Rate Profiling (GERP): This approach uses multi-species genome alignments to identify genomic sites that are strongly conserved over millions of years of evolution, as non-synonymous mutations at these sites have a high likelihood of being deleterious. (Davydov et al. 2010)\nSNP effect (SnpEff): This approach predicts the consequences of genomic variants on protein sequences and identifies loss of function and missense variants. (Cingolani et al. 2012).\n\n\n\n4 This webpage / document\nThis webpage can also be found in PDF format on github. Note that in the PDF format, code is not folded which will end up in a lengthy document, and the html looks aesthetically better ;). In both documents, you will find some of the scripts for the analysis performed in this study. Note that not all bioinformatic steps are put on here (only from inferring mutations onwards). You can find the complete set of analyses with their explanations in the github repo.\n\n\n\n\nBertorelle, Giorgio, Francesca Raffini, Mirte Bosse, Chiara Bortoluzzi, Alessio Iannucci, Emiliano Trucchi, Hernán E. Morales, and Cock van Oosterhout. 2022. “Genetic Load: Genomic Estimates and Applications in Non-Model Animals.” Nature Reviews Genetics 23 (8): 492–503. https://doi.org/10.1038/s41576-022-00448-x.\n\n\nCingolani, Pablo, Adrian Platts, Le Lily Wang, Melissa Coon, Tung Nguyen, Luan Wang, Susan J. Land, Xiangyi Lu, and Douglas M. Ruden. 2012. “A Program for Annotating and Predicting the Effects of Single Nucleotide Polymorphisms, SnpEff.” Fly 6 (2): 80–92. https://doi.org/10.4161/fly.19695.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper, Arend Sidow, and Serafim Batzoglou. 2010. “Identifying a High Fraction of the Human Genome to Be Under Selective Constraint Using GERP++.” Edited by Wyeth W. Wasserman. PLoS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025."
  },
  {
    "objectID": "qmd/1_snpeff.html#introduction",
    "href": "qmd/1_snpeff.html#introduction",
    "title": "2  SnpEff",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nSnpEff annotates genetic variants and predicts the functional effects. The output includes a VCF file with annotations that indicate what kind of mutation it is (e.g. introduction of a stop codon) and the predicted effect (low, moderate, high, modifier). In this study, we focus on high impact mutations, which include loss of function (LoF) and nonsense mediate decay (NMD) mutations."
  },
  {
    "objectID": "qmd/1_snpeff.html#methods",
    "href": "qmd/1_snpeff.html#methods",
    "title": "2  SnpEff",
    "section": "2.2 Methods",
    "text": "2.2 Methods\n\n2.2.1 Building the database\nAs the black grouse (Lyrurus tetrix) is no common model species with a pre-built database, a custom database was built from the annotation files in .gff format provided by Cantata Bio.\nTo build a custom database, five files are required: the gff file containing the gene annotation, the reference genome, and then three files containing information about the coding regions (cds.fa; a fasta file containing the coding regions only), the genes (genes.fa; a fasta file containing the genes only) and a file with the protein sequences (proteins.fa; a fasta file with the protein sequences). Two softwares were used to construct these three fasta files: gff3_to_fasta and AGAT.\ngff3_to_fasta -g data/genomic/refgenome/PO2979_Lyrurus_tetrix_black_grouse.annotation.gff \\\n    -f data/genomic/refgenome/PO2979_Lyrurus_tetrix_black_grouse.RepeatMasked.fasta -st cds -d complete -o data/genomic/refgenome/lyrurus_tetrix/cds.fa \n\ngff3_to_fasta -g data/genomic/refgenome/PO2979_Lyrurus_tetrix_black_grouse.annotation.gff \\\n    -f data/genomic/refgenome/PO2979_Lyrurus_tetrix_black_grouse.RepeatMasked.fasta -st gene -d complete -o data/genomic/refgenome/lyrurus_tetrix/genes.fa \nSimilarly, the protein sequences were constructed with AGAT\nagat_sp_extract_sequences.pl --gff data/genomic/refgenome/PO2979_Lyrurus_tetrix_black_grouse.annotation.gff -f \\\n    data/genomic/refgenome/PO2979_Lyrurus_tetrix_black_grouse.RepeatMasked.fasta -p -o \\\n    data/genomic/refgenome/lyrurus_tetrix/protein.fa\nThen, the database was built (and automatically checked).\njava -jar snpEff.jar build -gff3 -v data/genomic/refgenome/lyrurus_tetrix\nOnce the database is ready, we can run SnpEff to create the annotated vcf file.\njava -Xmx8g -jar snpEff.jar ann -stats  \\\n-no-downstream -no-intergenic -no-intron -no-upstream -no-utr -v \\\nlyrurus_tetrix data/genomic/intermediate/ltet_snps_filtered.vcf &gt; data/genomic/intermediate/snpef/ltet_ann_snp_output.vcf\n\n\n2.2.2 Ancestral alleles\nSnpEff annotates mutations according to the change from the reference allele to the focal allele. Hence, it assumes that the reference allele is the ‘better’ one and that a mutation that changes the transcription of this reference allele is detrimental. To allow this assumption to be better met, we used the ancestral genome as a reference, instead of the reference genome itself (i.e. we polarized the genome). This ancestral genome is constructed by cactus, and represents the most recent common ancestor between black grouse (L. tetrix) and Lagoplus leucura (white tailed ptarmigan). This way, any derived allele was assumed to be ‘deleterious’ compared to the ancestral allele, as opposed to a reference-non reference comparison.\n\n\n2.2.3 Filtering\nWe then used SnpSift to filter annotated mutations based on the four impact categories: modifier, low, moderate and high impact using the following commands.\n\n## High impact\nzcat output/ancestral/ltet_filtered_ann_aa.vcf.gz | java -jar src/SnpSift.jar filter \" ( ANN[*].IMPACT = 'HIGH' )\" &gt; data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_HIGH.vcf\ngzip data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_HIGH.vcf\n\n## Moderate\nzcat output/ancestral/ltet_filtered_ann_aa.vcf.gz | java -jar src/SnpSift.jar filter \" ( ANN[*].IMPACT = 'MODERATE')\" &gt; data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_moderate.vcf\ngzip data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_moderate.vcf\n\n## Low\nzcat output/ancestral/ltet_filtered_ann_aa.vcf.gz | java -jar src/SnpSift.jar filter \" ( ANN[*].IMPACT = 'LOW') \" &gt; data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_low.vcf\ngzip data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_low.vcf"
  },
  {
    "objectID": "qmd/1_snpeff.html#results",
    "href": "qmd/1_snpeff.html#results",
    "title": "2  SnpEff",
    "section": "2.3 Results",
    "text": "2.3 Results\nWe identified 5,341 high impact mutations:\n\n\n\nSnpEff annotation\n\n\nExisting of mostly LoF mutations and gained stop codons (non-mutually exclusive)\n\n\n\nDetailed SnpEff annotation\n\n\nThe mutations in the ‘high impact’ category were used to calculate individual genomic mutation load estimates."
  },
  {
    "objectID": "qmd/2_gerp.html#introduction",
    "href": "qmd/2_gerp.html#introduction",
    "title": "3  GERP",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nGERP++ annotates a focal genome based on evolutionary conserveration, where regions in the genome that show higher conservation across multiple different species are expected to face higher selective constraint. GERP score calculation, which indicate the reduction in the number of substitutions compared to neutral expectations, is done based on a multi-species alignment file. Higher GERP scores indicate higher evolutionary constraint. First, we create this MAF file using Progressive Cactus, then we calculate GERP scores genome-wide, and select genomic positions with SNPs in our population."
  },
  {
    "objectID": "qmd/2_gerp.html#methods",
    "href": "qmd/2_gerp.html#methods",
    "title": "3  GERP",
    "section": "3.2 Methods",
    "text": "3.2 Methods\n\n3.2.1 Creating the MAF\nWe use the publicly available 363 avian genomes multi-alignment file as a starting point, and then reduce this file to exclude species of the Neoaves clade. All the GERP analyses were done using the cactus container.\n\n\nCode\n### Remove subtrees that are not needed for ltet analysis\n\n#set cactus scratch directory\nCACTUS_SCRATCH=$(pwd)/scratch/\n\n# enter the container\napptainer shell --cleanenv \\\n  --fakeroot --overlay ${CACTUS_SCRATCH} \\\n  --bind ${CACTUS_SCRATCH}/tmp:/tmp,$(pwd) \\\n  --env PYTHONNOUSERSITE=1 \\\n  docker:quay.io/comparative-genomics-toolkit/cactus:v2.5.1 \n\n# get stats on the original 363-avian multi-alignment file\nhalStats data/genomic/intermediate/cactus/363-avian-2020.hal &gt; output/cactus/stats_original_363_hal.txt\n\n# copy the original file to then edit it\ncp data/genomic/intermediate/cactus/363-avian-2020.hal data/genomic/intermediate/cactus/363-avian-reduced.hal\n\n# remove subtrees to exclude neoaves\nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc1\nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc57 \nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc69 \nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc318 #starts with Heliornis_fulica\nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc319 #starts with Psophia_crepitans\nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc320 #starts with Charadrius_vociferus\nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc321 #starts with Opisthocomus_hoazin\nhalRemoveSubtree data/genomic/intermediate/cactus/363-avian-reduced.hal birdAnc322 #stars with birdAnc57, so the big chunk of passerines but also a \n\n#some individual ancestral genomes left to exclude\nhalRemoveGenome data/genomic/intermediate/cactus//363-avian-reduced.hal birdAnc322\nhalRemoveGenome data/genomic/intermediate/cactus//363-avian-reduced.hal birdAnc1\n\n# get stats of our subset of genomes\n\nhalStats data/genomic/intermediate/cactus/363-avian-reduced.hal &gt; output/cactus/stats_reduced_363_hal.txt\n\n#extract the reduced file \nhalExtract data/genomic/intermediate/cactus/363-avian-reduced.hal data/genomic/intermediate/cactus/363-avian-reduced.hal \n\n\nNext, we add two genomes: Lyrurus tetrix and Lagopus lecura, using the cactus prepare function. To add the Lagopus lecura genome to the hal file, the assembly needs to be downloaded from NCBI which can be found on NCBI: https://0-www-ncbi-nlm-nih-gov.brum.beds.ac.uk/datasets/genome/GCF_019238085.1/.\n\n\nCode\n# In this file, we will use the cactus-update-prepare function to create two scripts that will allow us to add two genomes to our dataset\n\n# set scratch directory\nCACTUS_SCRATCH=$(pwd)/scratch/\n\n#enter container\napptainer shell --cleanenv \\\n  --fakeroot --overlay ${CACTUS_SCRATCH} \\\n  --bind ${CACTUS_SCRATCH}/tmp:/tmp,$(pwd) \\\n  --env PYTHONNOUSERSITE=1 \\\n  src/containers/cactus_v2.5.1.sif \n\nsh\n\n#first add Lyrurus tetrix, branchlengths will be corrected in a later step\n\ncactus-update-prepare \\\n  add branch \\\n  --parentGenome birdAnc334 \\\n  --childGenome Tympanuchus_cupido \\\n  data/genomic/intermediate/cactus/363-avian-reduced.hal \\\n  scripts/2_cactus/input_ltet.txt \\\n  --cactus-prepare-options \\\n  '--alignCores 4' \\\n  --topBranchLength 0.01 \\ \n  --outDir scratch/tmp/steps-output \\\n  --jobStore scratch/tmp/js \\\n  --ancestorName AncX &gt; scripts/2_cactus/3_cactus_update_lyrurus_steps.sh \n\n# then add Lagopus leucura\n\ncactus-update-prepare \\\n  add branch \\\n  --parentGenome AncX \\\n  --childGenome Lyrurus_tetrix \\\n  data/genomic/intermediate/cactus/363-avian-reduced.hal \\\n  scripts/2_cactus/input_lleu.txt \\\n  --cactus-prepare-options \\\n  '--alignCores 4' \\\n  --topBranchLength 0.01 \\\n  --outDir scratch/tmp/steps-output \\\n  --jobStore scratch/tmp/js \\\n  --ancestorName AncY &gt; scripts/2_cactus/4_cactus_update_lagopus_steps.sh \n  \n\n\nThis is just the preparation step, and two files will be outputted that can be used to update the .hal file. This is what these files look like (and then they have to be executed).\n\n\nCode\n## Preprocessor\ncactus-preprocess scratch/tmp/js/0 scratch/tmp/steps-output/seq_file.in scratch/tmp/steps-output/seq_file.out --inputNames Lyrurus_tetrix --realTimeLogging --logInfo --retryCount 0 --maskMode none\n\n## Alignment\n\n### Round 0\ncactus-blast scratch/tmp/js/1 scratch/tmp/steps-output/seq_file.out scratch/tmp/steps-output/AncX.cigar --root AncX --restart \ncactus-align scratch/tmp/js/2 scratch/tmp/steps-output/seq_file.out scratch/tmp/steps-output/AncX.cigar scratch/tmp/steps-output/AncX.hal --root AncX  --maxCores 8 \nhal2fasta scratch/tmp/steps-output/AncX.hal AncX --hdf5InMemory &gt; scratch/tmp/steps-output/AncX.fa \n\n### Round 1\ncactus-blast scratch/tmp/js/3 scratch/tmp/steps-output/seq_file.out scratch/tmp/steps-output/birdAnc334.cigar --root birdAnc334 --includeRoot  \ncactus-align scratch/tmp/js/4 scratch/tmp/steps-output/seq_file.out scratch/tmp/steps-output/birdAnc334.cigar scratch/tmp/steps-output/birdAnc334.hal --root birdAnc334  --maxCores 4 --includeRoot  \n\n## Alignment update\nhalAddToBranch data/genomic/intermediate/cactus/363-avian-reduced.hal scratch/tmp/steps-output/AncX.hal scratch/tmp/steps-output/birdAnc334.hal birdAnc334 AncX Tympanuchus_cupido Lyrurus_tetrix 0.01 1.0 --hdf5InMemory \n\n## Alignment validation\nhalValidate --genome birdAnc334 data/genomic/intermediate/cactus/363-avian-reduced.hal --hdf5InMemory\nhalValidate --genome AncX data/genomic/intermediate/cactus/363-avian-reduced.hal --hdf5InMemory\nhalValidate --genome Tympanuchus_cupido data/genomic/intermediate/cactus/363-avian-reduced.hal --hdf5InMemory\nhalValidate --genome Lyrurus_tetrix data/genomic/intermediate/cactus/363-avian-reduced.hal --hdf5InMemory\n\n\nFor subsequent steps, the resulting hal file is converted to maf format per scaffold for both GERP++ and the neutral tree calculation. Note that we only focus on the 30 largest scaffolds of the black grouse genome which over &gt;95% of the genome, and only autosomal scaffolds.\nThis conversion is done within an R script\n\n\nCode\nhal =  \"data/genomic/intermediate/cactus/363-avian-reduced.hal\"\nlibrary(dplyr); library(data.table)\nscafs &lt;- fread(\"data/genomic/refgenome/30_largest.scafs.tsv\")\noutput_dir = \"output/cactus/maf_per_scaf\"\nsif = \"src/containers/cactus_v2.6.12.sif\"\nscratch = \"scripts/2_cactus/scratch\"\ntmp_js = \"scripts/2_cactus/scratch/tmp/js/wiggle\"\n\nhal_to_maf_per_scaf &lt;- function(hal, scaf, outdir, scratch, i, sif, tmp_js){\n    system(paste0('/vol/apptainer/bin/apptainer run --cleanenv --fakeroot --overlay ', scratch, ' --bind ', scratch, '/tmp:/tmp,', scratch, ' --env PYTHONNOUSERSITE=1 ', sif, ' cactus-hal2maf ', tmp_js, '/js_', i, ' --restart ', hal, ' ', outdir, '/maf_', i, '.maf --refGenome Lyrurus_tetrix --refSequence ', scaf, ' --dupeMode single --filterGapCausingDupes --chunkSize 1000000 --noAncestors'))\n}\n\nfor (i in 1:30){\n  hal_to_maf_per_scaf(hal = hal,\n  scaf = scafs$scaf[i],\n  outdir = output_dir,\n  scratch = scratch,\n  sif = sif,\n  i = i,\n  tmp_js = tmp_js)\n}\n\n\nLastly, the final phylogenetic tree has to be recalculated according to the updated tree, and the branch lengths have to be calculated in substitutions/site (rather than million years ago). This analysis can be found on github and will not be included here as it contains many small steps integrated with in a snakemake workflow.\n\n\n3.2.2 Calculate GERP scores\nGERP scores were calculated per scaffold using snakemake using the following rule:\n\n\nCode\nrule call_gerp:\n    input:\n      maf = \"output/cactus/maf_per_scaf/maf_{scaf}.maf\",\n      tree = \"output/tree_cactus_updated.txt\"\n    output:\n      rates = \"output/gerp/maf_{scaf}.maf.rates\"\n    params:\n      refname = \"Lyrurus_tetrix\"\n    log: \"logs/gerp_{scaf}.log\"\n    shell:\n      \"\"\"\n      gerpcol -t {input.tree} -f {input.maf} -e {params.refname} -j -z -x \".rates\" &&gt; {log}\n      \"\"\"\n\n\n\n\n3.2.3 Overlap GERP scores with SNPs\nWe are only interested in genomic locations where SNPs were found in our population. Therefore, we convert our VCF file and GERP files to bed format to overlap the SNPs with bedtools using the following commands (integrated in snakemake):\n\n\nCode\nrule vcf_to_bed:\n    input:\n        vcf = \"output/ancestral/ltet_filtered_ann_aa.vcf.gz\"\n    output:\n        bed = \"output/ancestral/ltet_filtered_ann_aa.bed\"\n    shell:\n        \"\"\"\n        convert2bed -i vcf &lt; {input.vcf} &gt; {output.bed}\n\nrule gerp_to_bed:\n    input:\n        rates = \"output/gerp/maf_per_scaf/biggest_30/maf_{nscaf}.maf.rates\"\n    output:\n        bed = \"output/gerp/beds/gerp_scaf_{nscaf}.bed\"\n    params:\n        outdir = \"output/gerp/beds\"\n    log: \"logs/gerp_to_bed_{nscaf}\"\n    shell:\n        \"\"\"\n        Rscript --vanilla scripts/6_snpeff_gerp/2_gerp/gerp_to_bed.R {input.rates} {output.bed} {params.outdir} &&gt; {log}\n        \"\"\"\n\nrule bed_overlap_snps:\n    input:\n        bed = \"output/gerp/beds/gerp_scaf_{nscaf}.bed\",\n        snps = \"output/ancestral/ltet_filtered_ann_aa.bed\"\n    output:\n        tsv = \"output/gerp/beds/gerp_overlapSNP_scaf_{nscaf}.tsv.gz\"\n    params:\n        tsv = \"output/gerp/beds/gerp_overlapSNP_scaf_{nscaf}.tsv\"    \n    shell:\n        \"\"\"\n        bedtools intersect \\\n        -a {input.bed} \\\n        -b {input.snps} \\\n        -wa -wb |\n        cut -f 6-10 --complement &gt; {params.tsv}\n\n        gzip {params.tsv}\n        \"\"\"\n\n\nAs these files are still very large, we loop over scaffolds within snakemake with an R script to count the number of mutations per individual per scaffold, both in homozygosity and heterozygosity using the following R formula (which is used for each scaffold separately and outputs a tsv file used for calculating mutation load in the next script).\n\n\nCode\ncalculate_gerp_load &lt;- function(gerp_vcf, scafno){\n  ## metadata on filenames and ids\n  filenames &lt;- fread(\"data/genomic/raw/metadata/idnames.fam\")\n  ids &lt;- fread(\"data/genomic/raw/metadata/file_list_all_bgi_clean.csv\")\n  \n  #merge\n  idnames &lt;- left_join(filenames[,c(\"V1\")], ids[,c(\"loc\", \"id\")], by = c(\"V1\" = \"loc\"))\n  \n  file &lt;- read_tsv(gerp_vcf, col_names = c(\"chr\", \"start\", \"pos\", \"neutral_rate_n\", \"rs_score\", \"ancestral\", \"derived\", \"qual\", \"info\",\"format\", idnames$id) )#rename columns\n  \n  # only get GT info, PL and DP are filtered by already anyway \n  gt &lt;- c(11:ncol(file))\n  select_n3 &lt;- function(x){x = substr(x,1,3)}\n  file[gt] &lt;- lapply(file[gt], select_n3)\n  \n  # replace genotype with RS value but separate per zygosity, do per ID\n  gerp_load &lt;- list()\n  for( id in 11:ncol(file)){\n    subset_id &lt;- file[,c(1:10, id)]\n    subset_id &lt;- subset_id %&gt;% mutate(gerp_cat = as.factor(case_when(\n        rs_score &lt; 0 ~ \"&lt; 0\", #changed\n        rs_score &gt;= 0 & rs_score &lt; 1 ~ \"0-1\",\n        rs_score &gt;= 1 & rs_score &lt; 2 ~ \"1-2\",\n        rs_score &gt;= 2 & rs_score &lt; 3 ~ \"2-3\",\n        rs_score &gt;= 3 & rs_score &lt; 4 ~ \"3-4\",\n        rs_score &gt;= 4 ~ \"4-5\"\n    )))\n    gerp_load_id &lt;- list()\n    for (i in c(\"&lt; 0\", \"0-1\", \"1-2\", \"2-3\", \"3-4\", \"4-5\")){#changed\n        cat_subset &lt;- subset(subset_id, gerp_cat == i)\n        het_data &lt;- subset(cat_subset, cat_subset[[11]] == \"1/0\" | cat_subset[[11]] == \"0/1\")\n        hom_data &lt;- subset(cat_subset, cat_subset[[11]] == \"1/1\")\n        n_genotyped &lt;- nrow(cat_subset) - nrow(subset(cat_subset, cat_subset[[11]] == \"./.\"))\n        n_total &lt;- nrow(cat_subset)\n        df &lt;- data.frame(id = colnames(file[id]),\n                         gerp_cat = i,\n                         scafno = scafno,\n                         n_total = n_total,\n                         n_genotyped = n_genotyped,\n                         het_data = nrow(het_data),\n                         hom_data = nrow(hom_data))\n        \n        gerp_load_id[[i]] &lt;- df\n        \n        }\n        gerp_load_id &lt;- do.call(rbind.data.frame, gerp_load_id)\n        rownames(gerp_load_id) &lt;- NULL\n    \n    gerp_load[[id]] &lt;- gerp_load_id\n    }\n  gerp_load &lt;- do.call(rbind.data.frame, gerp_load)\n\n    return(gerp_load)}"
  },
  {
    "objectID": "qmd/2_gerp.html#results",
    "href": "qmd/2_gerp.html#results",
    "title": "3  GERP",
    "section": "3.3 Results",
    "text": "3.3 Results\nWe identified 413,489 mutations with a GERP score higher than or equal to 4: \nThese mutations were used to calculate mutation load in the next script."
  },
  {
    "objectID": "qmd/3_load.html#introduction",
    "href": "qmd/3_load.html#introduction",
    "title": "4  Calculating mutation load",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThere are various types of load and in general, mutation load can be divided between potential and realized load. Realized load (also known as expressed load) only includes the deleterious mutations that are expressed in the individual (Bertorelle et al. 2022). Potential load (also known as inbreeding or masked load) is the fitness reduction due to deleterious mutations, of which not all are expressed on an individual level and therefore quantifies recessive deleterious mutations that could be expressed in future generations (Bertorelle et al. 2022).\nHowever, to be able to distinguish between realized and potential load, you need to know dominance coefficients, which we do not. Therefore, we focus on homozygous and heterozygous load instead, which consists of mutations in homo- and heterozygosity in an individual instead. The total load sums the number of mutations contributing to the load, where heterozygous mutations are counted ones (one per allele) and homozygous mutations twice (one per allele).\nFrom here on onwards, the majority of analyses are computed within R (instead of bash scripts/snakemake)."
  },
  {
    "objectID": "qmd/3_load.html#mutation-load-snpeff",
    "href": "qmd/3_load.html#mutation-load-snpeff",
    "title": "4  Calculating mutation load",
    "section": "4.2 Mutation load (SnpEff)",
    "text": "4.2 Mutation load (SnpEff)\nHere, we load in the .vcf file outputted by SnpSift with only high impact SnpEff mutations, add column names, include only the 29 largest autosomal scaffolds, exclude warning messages, convert the genotype columns into only 1/1, 1/0, 0/1, 0/0 and ./. values, calculate load per individual, and then merge the load estimates of all individuals together.\n\n\nCode\n### load packages ###\npacman::p_load(dplyr, data.table)\n\n### function to calculate load ###\ncalculate_load_snpeff &lt;- function(vcf, output_vcf, loadtype){\n  ## metadata on filenames and ids\n  filenames &lt;- fread(\"data/genomic/raw/metadata/idnames.fam\")\n  ids &lt;- fread(\"data/genomic/raw/metadata/file_list_all_bgi_clean.csv\")\n  \n  #merge\n  idnames &lt;- left_join(filenames[,c(\"V1\")], ids[,c(\"loc\", \"id\")], by = c(\"V1\" = \"loc\"))\n  \n  names(vcf) &lt;- c(c(\"CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\", \"QUAL\", \"FILTER\", \"INFO\",\"FORMAT\"), idnames$id)#rename columns\n\n  # only select 29 largest autosomal scaffolds\n  scaf &lt;- fread(\"data/genomic/refgenome/30_largest.scafs.tsv\")\n  scaf$scaf &lt;- gsub(\":\", \";\", scaf$scaf)\n  scaf$scaf &lt;- gsub(\"\\\\.\", \"=\", scaf$scaf)\n  scaf &lt;- subset(scaf, scaf_no != 4)\n  \n  vcf &lt;- subset(vcf, CHROM %in% scaf$scaf)\n  \n  # exclude warning messages\n  \n  vcf &lt;- subset(vcf, !grepl(\"WARNING\", INFO))\n  \n  # only get GT info, PL and DP are filtered by already anyway \n  gt &lt;- c(10:ncol(vcf))\n  select_n3 &lt;- function(x){x = substr(x,1,3)}\n  vcf[gt] &lt;- lapply(vcf[gt], select_n3)\n  \n  # calculate load\n  load &lt;- list()\n  # loop over ids\n  for( id in 10:(ncol(vcf))){\n    # subset per id\n    subset_id &lt;- vcf[,c(1:9, id)]\n    \n    # filter for snps in het and hom\n    het_data &lt;- subset(subset_id, subset_id[[10]] == \"1/0\" | subset_id[[10]] == \"0/1\")\n    hom_data &lt;- subset(subset_id, subset_id[[10]] == \"1/1\")\n    \n    # count amount of snps in het and hom\n    het_load_sum &lt;- nrow(het_data)\n    hom_load_sum &lt;- nrow(hom_data)\n    \n    # count no of snps successfully genotyped\n    n_genotyped &lt;- nrow(subset_id) - nrow(subset(subset_id, subset_id[[10]] == \"./.\"))\n    n_total &lt;- nrow(subset_id)\n    \n    # collect data in df\n    df &lt;- data.frame(id = colnames(vcf[id]),\n                     n_total = n_total,\n                     n_genotyped = n_genotyped,\n                     het_load = het_load_sum / n_genotyped,\n                     hom_load = hom_load_sum / n_genotyped,\n                     total_load = (het_load_sum*0.5 + hom_load_sum) / n_genotyped,\n                     loadtype = loadtype)\n    load[[id]] &lt;- df\n  }\n  # convert list to df\n  load &lt;- do.call(rbind.data.frame, load)\n  \n  if(output_vcf == TRUE){\n    out &lt;- list(load = load, vcf = vcf)\n    return(out)\n  }\n  \n  if(output_vcf==FALSE){\n    return(load)}\n}\n\n##### load high impact mutations (filtered by snpsift) #####\n\nhigh &lt;- read.table(\"data/genomic/intermediate/snpef/ltet_ann_aa_snp_output_HIGH.vcf.gz\")\n\n## calculate load \n# in this function, we give the columns names, filter for only the largest 29 autosomal scaffolds and exclude annotations with warning messages\n\nhigh_load &lt;- calculate_load_snpeff(high, output_vcf = TRUE, loadtype = \"high\")"
  },
  {
    "objectID": "qmd/3_load.html#mutation-load-gerp",
    "href": "qmd/3_load.html#mutation-load-gerp",
    "title": "4  Calculating mutation load",
    "section": "4.3 Mutation load (GERP)",
    "text": "4.3 Mutation load (GERP)\nHere, we load in the .bed files that contain GERP scores from SNPs, filter for those with GERP values &gt;= 4, add column names, convert the genotype columns into only 1/1, 1/0, 0/1, 0/0 and ./. values, calculate load per individual, and then merge the load estimates of all individuals together. Note this step is quite time-intensive as the .bed files are large in filesize!\n\n\nCode\n# load in all bed files with gerp scores that overlap a SNP\ngerp_snp_scafs &lt;- list.files(path = \"output/gerp/beds\", pattern = \"gerp_overlapSNP*\", full.names = T)\ngerp_snp_scafs &lt;- gerp_snp_scafs[-22] #empty, scaffold 29 has no SNPs with gerp scores\n\ngerp_snp &lt;- data.frame()\nfor (i in 1:length(gerp_snp_scafs)){\n  scaf &lt;- read.table(gerp_snp_scafs[i])\n  scaf &lt;- scaf %&gt;% filter(V5 &gt;= 4)\n  gerp_snp &lt;- rbind(gerp_snp, scaf)\n}\n\n## function to calculate load\n\ncalculate_load_gerp &lt;- function(vcf, output_vcf, loadtype){\n  \n  ## metadata on filenames and ids\n  filenames &lt;- fread(\"data/genomic/raw/metadata/idnames.fam\")\n  ids &lt;- fread(\"data/genomic/raw/metadata/file_list_all_bgi_clean.csv\")\n  \n  #merge\n  idnames &lt;- left_join(filenames[,c(\"V1\")], ids[,c(\"loc\", \"id\")], by = c(\"V1\" = \"loc\"))\n  \n  names(vcf) &lt;- c(\"chr\", \"start\", \"pos\", \"neutral_rate_n\", \"rs_score\", \"ancestral\", \"derived\", \"qual\", \"info\",\"format\", idnames$id) #rename columns\n  \n  # only get GT info, PL and DP are filtered by already anyway \n  gt &lt;- c(11:ncol(vcf))\n  select_n3 &lt;- function(x){x = substr(x,1,3)}\n  vcf[gt] &lt;- lapply(vcf[gt], select_n3)\n  \n  # calculate load\n  load &lt;- list()\n  # loop over ids\n  for( id in 11:(ncol(vcf))){\n    # subset per id\n    subset_id &lt;- vcf[,c(1:10, id)]\n    \n    # filter for snps in het and hom\n    het_data &lt;- subset(subset_id, subset_id[[11]] == \"1/0\" | subset_id[[11]] == \"0/1\")\n    hom_data &lt;- subset(subset_id, subset_id[[11]] == \"1/1\")\n    \n    # count amount of snps in het and hom\n    het_load_sum &lt;- nrow(het_data)\n    hom_load_sum &lt;- nrow(hom_data)\n    \n    # count no of snps successfully genotyped\n    n_genotyped &lt;- nrow(subset_id) - nrow(subset(subset_id, subset_id[[11]] == \"./.\"))\n    n_total &lt;- nrow(subset_id)\n    \n    # collect data in df\n    df &lt;- data.frame(id = colnames(vcf[id]),\n                     n_total = n_total,\n                     n_genotyped = n_genotyped,\n                     het_load = het_load_sum / n_genotyped,\n                     hom_load = hom_load_sum / n_genotyped,\n                     total_load = (het_load_sum*0.5 + hom_load_sum) / n_genotyped,\n                     loadtype = loadtype)\n    load[[id]] &lt;- df\n  }\n  # convert list to df\n  load &lt;- do.call(rbind.data.frame, load)\n  \n  if(output_vcf == TRUE){\n    out &lt;- list(load = load, vcf = vcf)\n    return(out)\n  }\n  \n  if(output_vcf==FALSE){\n  return(load)}\n}\n\n## calculate load\ngerp_45 &lt;- calculate_load_gerp(gerp_snp, output_vcf = TRUE, loadtype = \"gerp45\") \ngerp &lt;- gerp_45_load_check$vcf\n\n\n\n4.3.1 Combine loads\nNote: the analyses done above was also done for other mutation categories, e.g. low and moderate impact classes and GERP scores between 3-4. All load scores are then combined into a single file:\n\n\nCode\nload &lt;- rbind(high_load$load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")] ,\n              moderate_load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")], \n              low_load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")],\n              lof_load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")],\n              missense_load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")], \n              gerp_34_load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")], \n              gerp_45_load[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")])\n\nsave(load, file = \"output/load/all_loads_combined_da_nosex_29scaf.RData\")\nwrite.table(load, file = \"output/load/all_loads_combined_da_nosex_29scaf.tsv\", sep=\"\\t\", row.names = F)\n\n\nWe can then calculate the correlation between the two load estimates and test for lek effects on load.\n\n\nCode\nlibrary(dplyr)\nload(file = \"../output/load/all_loads_combined_da_nosex_29scaf.RData\")\n\ncor.test(load$total_load[which(load$loadtype == \"gerp45\")], load$total_load[which(load$loadtype == \"high\")])\n\n\n\n    Pearson's product-moment correlation\n\ndata:  load$total_load[which(load$loadtype == \"gerp45\")] and load$total_load[which(load$loadtype == \"high\")]\nt = 1.7867, df = 188, p-value = 0.07559\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.01338135  0.26666622\nsample estimates:\n      cor \n0.1292181 \n\n\nCode\n### Test for lek effects ####\nload(\"../data/phenotypes/phenotypes_lifetime.RData\")\npheno_load &lt;- left_join(pheno_wide, load, by = \"id\")\n\nsummary(lm(total_load ~ site, data = subset(pheno_load, loadtype == \"gerp45\")))\n\n\n\nCall:\nlm(formula = total_load ~ site, data = subset(pheno_load, loadtype == \n    \"gerp45\"))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.426e-03 -2.162e-04  4.882e-05  3.303e-04  1.308e-03 \n\nCoefficients:\n              Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)  1.519e-01  6.684e-05 2272.733   &lt;2e-16 ***\nsiteLEH     -8.574e-05  1.171e-04   -0.732    0.465    \nsiteNYR     -5.353e-05  9.711e-05   -0.551    0.582    \nsiteSAA     -5.700e-05  1.200e-04   -0.475    0.635    \nsiteTEE      4.854e-05  1.337e-04    0.363    0.717    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0005177 on 185 degrees of freedom\nMultiple R-squared:  0.006348,  Adjusted R-squared:  -0.01514 \nF-statistic: 0.2955 on 4 and 185 DF,  p-value: 0.8807\n\n\nCode\nsummary(lm(total_load ~ site, data = subset(pheno_load, loadtype == \"high\")))\n\n\n\nCall:\nlm(formula = total_load ~ site, data = subset(pheno_load, loadtype == \n    \"high\"))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0098650 -0.0020586  0.0003317  0.0020222  0.0083354 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.1653480  0.0003800 435.179   &lt;2e-16 ***\nsiteLEH     -0.0001091  0.0006656  -0.164    0.870    \nsiteNYR     -0.0001785  0.0005521  -0.323    0.747    \nsiteSAA     -0.0009170  0.0006820  -1.344    0.180    \nsiteTEE     -0.0005430  0.0007599  -0.715    0.476    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.002943 on 185 degrees of freedom\nMultiple R-squared:  0.01131,   Adjusted R-squared:  -0.01007 \nF-statistic: 0.5289 on 4 and 185 DF,  p-value: 0.7146\n\n\n\n\n\n\nBertorelle, Giorgio, Francesca Raffini, Mirte Bosse, Chiara Bortoluzzi, Alessio Iannucci, Emiliano Trucchi, Hernán E. Morales, and Cock van Oosterhout. 2022. “Genetic Load: Genomic Estimates and Applications in Non-Model Animals.” Nature Reviews Genetics 23 (8): 492–503. https://doi.org/10.1038/s41576-022-00448-x."
  },
  {
    "objectID": "qmd/4_models.html#introduction",
    "href": "qmd/4_models.html#introduction",
    "title": "5  Modelling fitness",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nNow that we have mutation load (total, homozygous and heterozygous) estimates for each individual, based on SnpEff and GERP, we can model their effects on lifetime mating success (LMS).\nHere, we build three sets of models:\n\nThe effect of total load on LMS\nThe effects of homo- and heterozygous load on LMS\nThe direct and indirect effects of total load on mating success through the sexual traits\n\nWe use Bayesian GLMMs using the R package ‘brms’ to compute these models"
  },
  {
    "objectID": "qmd/4_models.html#methods",
    "href": "qmd/4_models.html#methods",
    "title": "5  Modelling fitness",
    "section": "5.2 Methods",
    "text": "5.2 Methods\n\n5.2.1 Total load\nThe general structure of the total load models is as follows:\nLMS ~ scale(total_load) + core + (1|site)\nThis is what the script for each load type looks like:\n\n\nCode\n# load pheno data lifetime\nload(file = \"output/load/pheno_loads_lifetime.RData\")\n\n# load mutation load measures\nload(\"output/load/all_loads_combined_da_nosex_29scaf_plus_per_region.RData\") #loads no sex chr only 30 scaf\n\n# subset only the relevant method/loadtype\nload &lt;- load_per_region %&gt;% filter(loadtype == method)\n\n# combine\npheno_wide_load &lt;- left_join(pheno_wide, load, by = \"id\")\npheno_wide_load &lt;- subset(pheno_wide_load, !is.na(total_load)) #some ids without genotypes, excluded for wgr\n\n#### model ####\nbrm_load_t &lt;- brm(LMS_min ~ scale(total_load) + core + (1|site), data = pheno_wide_load,\n                  family = \"zero_inflated_poisson\",\n                  prior = prior(normal(0,1), class = b),\n                  cores =8, control = list(adapt_delta = 0.99, max_treedepth = 15),\n                  iter = iter, thin = thin, warmup = warm, seed = 1908)\n\nsave(brm_load_t, file = out)\n\n\nWe can check out the performance of each model using the following loop:\n\n\nCode\noutput_total &lt;- list.files(path = \"output/models/total_hom_het/\",\n                         pattern = \"lms*\", full.names=T)\n\ndiagnose_summary &lt;- list()\nfor (i in 1:length(output)){\n  #load fit\n  load(file = output[[i]])\n  #get posteriors\n  posterior &lt;- as.array(fit)\n  log_ps &lt;- log_posterior(fit)\n  nuts &lt;- nuts_params(fit) #divergence\n  #get only beta and sd\n  betas &lt;- variables(fit)[grep(\"b_\", variables(fit))]\n  sd &lt;- variables(fit)[grep(\"sd_\", variables(fit))]\n  \n  #global patterns in divergence\n  diverge_beta &lt;- mcmc_parcoord(posterior, np = nuts, pars= betas)\n  diverge_sd &lt;- mcmc_parcoord(posterior, np = nuts, pars= sd)\n  \n  #identify collinearity between parameters\n  collin_beta &lt;- mcmc_pairs(posterior, np = nuts, pars= betas)\n  collin_sd &lt;- mcmc_pairs(posterior, np = nuts, pars= sd)\n  \n  #traceplot\n  trace_beta &lt;- mcmc_trace(posterior, pars = betas, np = nuts)\n  trace_sd &lt;- mcmc_trace(posterior, pars = sd, np = nuts)\n  \n  #rhat\n  rhat &lt;- mcmc_rhat(brms::rhat(fit))\n  \n  #effective sample size\n  neff &lt;- mcmc_neff(neff_ratio(fit))\n  \n  #autocorrelation\n  autocor_beta &lt;- mcmc_acf(posterior, pars = betas)\n  autocor_sd &lt;- mcmc_acf(posterior, pars=sd)\n  \n  #quick glance results\n  areas &lt;- mcmc_areas(fit, pars=betas)\n  \n  #combine in list\n  diagnosis &lt;- list(diverge_beta = diverge_beta, \n                    diverge_sd = diverge_sd, \n                    collin_beta = collin_beta, \n                    collin_sd = collin_sd, \n                    trace_beta = trace_beta, \n                    trace_sd = trace_sd, \n                    rhat = rhat, \n                    neff = neff, \n                    autocor_beta = autocor_beta, \n                    autocor_sd = autocor_sd,\n                    areas = areas)\n  \n  \n  modelname &lt;- sub(\".*/\", \"\", output[i]) \n  modelname &lt;- sub(\".RData\", \"\", modelname)\n  \n  # add to summary\n  diagnose_summary[[modelname]] &lt;- diagnosis\n}\n\n\nThis is what these plots look like for total GERP load effects:\nDiagnostics GERP total load model\n\n\n5.2.2 Hom and het load\nThe general structure of the homozygous and heterozygous load models is as follows:\nLMS ~ scale(het_load) + scale(hom_load) + core + (1|site)\nThis is what the script for each load type looks like:\n\n\nCode\n# load pheno data lifetime\nload(file = \"output/load/pheno_loads_lifetime.RData\")\n\n# load mutation load measures\nload(\"output/load/all_loads_combined_da_nosex_29scaf_plus_per_region.RData\") #loads no sex chr only 30 scaf\n\n# subset only the relevant method/loadtype\nload &lt;- load_per_region %&gt;% filter(loadtype == method)\n\n# combine\npheno_wide_load &lt;- left_join(pheno_wide, load, by = \"id\")\npheno_wide_load &lt;- subset(pheno_wide_load, !is.na(total_load)) #some ids without genotypes, excluded for wgr\n\n#### model ####\nbrm_load_het_hom &lt;- brm(LMS_min ~ scale(het_load) + scale(hom_load) + core + (1|site), data = pheno_wide_load,\n                  family = \"zero_inflated_poisson\",\n                  prior = prior(normal(0,1), class = b),\n                  cores =8, control = list(adapt_delta = 0.99, max_treedepth = 15),\n                  iter = iter, thin = thin, warmup = warm, seed = 1908)\n\nsave(brm_load_het_hom, file = out)\n\n\nThe same diagnostics were applied to each model.\n\n\n5.2.3 Direct and indirect effects\nNext, we build models that are based on annual values. There are two sets of models: the first quantifies the effect of load on the six sexual traits (attendance, fighting rate, centrality, lyre size, blue chroma and red eye comb size) in six separate models. The second set analyses the effect of the six traits and load on annual mating success (MS).\nThe direct effect is the effect of load on MS while correcting for all mediators. The indirect effect is calculated using a mediation analysis, where this effect is calculated as the product of the effect of the predictor (the total load) on the mediator (the sexual trait) and the effect of the mediator on the response variable (AMS).\n\n5.2.3.1 Set 1: load on traits\nFor each trait, we run this model:\nscale(trait) ~ scale(total_load) + age_cat + (1|year) + (1|site/id)\n\n\nCode\n### load data ###\n\nload(file = \"data/phenotypes/phenotypes_annual.RData\")\n\n# load mutation load measures\nload(\"output/load/all_loads_combined_da_nosex_29scaf_plus_per_region.RData\") #loads no sex chr only 30 scaf\n\n# subset only the relevant method/loadtype\nload &lt;- load_per_region %&gt;% filter(loadtype == method)\n\n# merge files\npheno &lt;- left_join(pheno_long, load, by = \"id\")\npheno$born &lt;- pheno$year - pheno$age\npheno &lt;- pheno %&gt;% mutate(age_cat = as.factor(case_when(age == 1 ~ \"yearling\", age &gt; 1  ~ \"adult\")))\n\n### modelling ####\nformula &lt;- formula(paste0(\"scale(\", response, \") ~ scale(total_load) + age_cat + (1|year) + (1|site/id)\"))\n\nfit &lt;- brm(formula,\n            family = \"gaussian\",\n           data = pheno, \n           cores =8,\n           control = list(adapt_delta = 0.99, max_treedepth = 15),\n           prior = prior(normal(0,1), class = b),\n           iter = iterations, \n           thin = thin, warmup = burn)\n\nsave(fit, file = out)\n\n\n\n\n5.2.3.2 Set 2: trait + load on MS\nThen, for both approaches we run the following model:\nMS ~ scale(total_load) + scale(lyre) + scale(eyec) + scale(blue) + scale(dist) + scale(attend) + scale(fight) + age_cat + (1|year) + (1|site/id)\n\n\nCode\n### load data ###\n\nload(file = \"data/phenotypes/phenotypes_annual.RData\")\n\n# load mutation load measures\nload(\"output/load/all_loads_combined_da_nosex_29scaf_plus_per_region.RData\") #loads no sex chr only 30 scaf\n\n# subset only the relevant method/loadtype\nload &lt;- load_per_region %&gt;% filter(loadtype == method)\n\n# merge files\npheno &lt;- left_join(pheno_long, load, by = \"id\")\npheno$born &lt;- pheno$year - pheno$age\n\npheno &lt;- pheno %&gt;% mutate(age_cat = as.factor(case_when(age == 1 ~ \"yearling\", age &gt; 1  ~ \"adult\")))\n\n### modelling ####\nformula &lt;- formula(\"MS ~ scale(total_load) + scale(lyre) + scale(eyec) + scale(blue) + scale(dist) + scale(attend) + scale(fight) + age_cat + (1|year) + (1|site/id)\")\n\nfit &lt;- brm(formula,\n           family = \"zero_inflated_poisson\",\n           data = pheno, \n           cores =8,\n           control = list(adapt_delta = 0.99, max_treedepth = 15),\n           prior = prior(normal(0,1), class = b),\n           iter = iterations, \n           thin = thin, warmup = burn)\n\nsave(fit, file = out)\n\n\n\n\n5.2.3.3 Direct / indirect effects\nThe direct/indirect effects are then calculated after loading in all model outputs:\n\n\nCode\n### load packages ####\n\npacman::p_load(brms, bayesplot, dplyr, data.table)\n\n### load models ###\n\n#### gerp ####\nload(file = \"output/models/annual/traits/model_attend_gerp45.RData\")\nfit_gerp_attend &lt;- fit\nload(file = \"output/models/annual/traits/model_fight_gerp45.RData\")\nfit_gerp_fight &lt;- fit\nload(file = \"output/models/annual/traits/model_dist_gerp45.RData\")\nfit_gerp_dist &lt;- fit\nload(file = \"output/models/annual/traits/model_eyec_gerp45.RData\")\nfit_gerp_eyec &lt;- fit\nload(file = \"output/models/annual/traits/model_blue_gerp45.RData\")\nfit_gerp_blue &lt;- fit\nload(file = \"output/models/annual/traits/model_lyre_gerp45.RData\")\nfit_gerp_lyre &lt;- fit\n\nload(file = \"output/models/annual/ams/model_trait_ams_gerp45.RData\")\nfit_gerp_ams &lt;- fit\n\nrm(fit)\n\n### snpeff \nload(file = \"output/models/annual/traits/model_attend_high.RData\")\nfit_high_attend &lt;- fit\nload(file = \"output/models/annual/traits/model_fight_high.RData\")\nfit_high_fight &lt;- fit\nload(file = \"output/models/annual/traits/model_dist_high.RData\")\nfit_high_dist &lt;- fit\nload(file = \"output/models/annual/traits/model_eyec_high.RData\")\nfit_high_eyec &lt;- fit\nload(file = \"output/models/annual/traits/model_blue_high.RData\")\nfit_high_blue &lt;- fit\nload(file = \"output/models/annual/traits/model_lyre_high.RData\")\nfit_high_lyre &lt;- fit\n\nload(file = \"output/models/annual/ams/model_trait_ams_high.RData\")\nfit_high_ams &lt;- fit\n\nrm(fit)\n\n### indirect effects loop #####\nget_indirect &lt;- function(mediator, method, trait_model, ams_model){\n  treatment = \"b_scaletotal_load\"\n  path1 &lt;- as_draws_df(trait_model, variable =treatment)\n  path1 &lt;- path1$b_scaletotal_load\n  \n  path2 &lt;- as_draws_df(ams_model, variable = mediator)\n  path2 &lt;- unlist(c(path2[,1]))\n  \n  indirect &lt;- path1*path2\n  \n  direct &lt;- as_draws_df(ams_model, variable =treatment)\n  direct &lt;- direct$b_scaletotal_load\n  \n  total &lt;- indirect + direct\n  \n  effect_attend &lt;- data.frame(treatment = treatment,\n                              mediator = mediator,\n                              method = method,\n                              indirect_median = round(median(indirect), 2),\n                              indirect_lower = round(quantile(indirect, probs = c(.025)), 2),\n                              indirect_upper = round(quantile(indirect, probs = c(.975)), 2),\n                              direct_median = round(median(direct), 2),\n                              direct_lower = round(quantile(direct, probs = c(.025)), 2),\n                              direct_upper = round(quantile(direct, probs = c(.975)), 2),\n                              total_median = round(median(total), 2),\n                              total_lower = round(quantile(total, probs = c(.025)), 2),\n                              total_upper = round(quantile(total, probs = c(.975)), 2),\n                              path1_median = round(median(path1), 2),\n                              path1_lower = round(quantile(path1, probs = c(.025)), 2),\n                              path1_upper = round(quantile(path1, probs = c(.975)), 2),\n                              path2_median = round(median(path2), 2),\n                              path2_lower = round(quantile(path2, probs = c(.025)), 2),\n                              path2_upper = round(quantile(path2, probs = c(.975)), 2),\n                              indirect_lower_80 = round(quantile(indirect, probs = c(.1)), 2),\n                              indirect_upper_80 = round(quantile(indirect, probs = c(.9)), 2),\n                              direct_lower_80 = round(quantile(direct, probs = c(.1)), 2),\n                              direct_upper_80 = round(quantile(direct, probs = c(.9)), 2))\n  \n  return(effect_attend)\n}\n\neffects &lt;- data.frame(rbind(get_indirect(mediator=\"b_scalelyre\", method = \"gerp45\", \n                                         trait_model=fit_gerp_lyre, ams_model = fit_gerp_ams),\n                            get_indirect(mediator=\"b_scaleeyec\",  method = \"gerp45\", \n                                         trait_model=fit_gerp_eyec, ams_model = fit_gerp_ams),\n                            get_indirect(mediator=\"b_scaleblue\",  method = \"gerp45\", \n                                         trait_model=fit_gerp_blue, ams_model = fit_gerp_ams),\n                            get_indirect(mediator=\"b_scaleattend\",  method = \"gerp45\", \n                                         trait_model=fit_gerp_attend, ams_model = fit_gerp_ams),\n                            get_indirect(mediator=\"b_scalefight\",  method = \"gerp45\", \n                                         trait_model=fit_gerp_fight, ams_model = fit_gerp_ams),\n                            get_indirect(mediator=\"b_scaledist\",  method = \"gerp45\", \n                                         trait_model=fit_gerp_dist, ams_model = fit_gerp_ams),\n                            get_indirect(mediator=\"b_scalelyre\", method = \"high\", \n                                         trait_model=fit_high_lyre, ams_model = fit_high_ams),\n                            get_indirect(mediator=\"b_scaleeyec\",  method = \"high\", \n                                         trait_model=fit_high_eyec, ams_model = fit_high_ams),\n                            get_indirect(mediator=\"b_scaleblue\",  method = \"high\", \n                                         trait_model=fit_high_blue, ams_model = fit_high_ams),\n                            get_indirect(mediator=\"b_scaleattend\",  method = \"high\", \n                                         trait_model=fit_high_attend, ams_model = fit_high_ams),\n                            get_indirect(mediator=\"b_scalefight\",  method = \"high\", \n                                         trait_model=fit_high_fight, ams_model = fit_high_ams),\n                            get_indirect(mediator=\"b_scaledist\",  method = \"high\", \n                                         trait_model=fit_high_dist, ams_model = fit_high_ams)))\n\n\nwrite.csv(effects, file = \"output/models/annual/direct_indirect_summary.csv\", quote=F, row.names = F)"
  },
  {
    "objectID": "qmd/4_models.html#results",
    "href": "qmd/4_models.html#results",
    "title": "5  Modelling fitness",
    "section": "5.3 Results",
    "text": "5.3 Results\nWe find significant effects of total GERP and total SnpEff load:\n\n\n\nTotal load results\n\n\n\n\nCode\nlibrary(readxl); library(dplyr); library(kableExtra)\ntotals &lt;- read.csv(\"../output/models/intervals/total_gerp45_high.csv\")\ntotals %&gt;% kbl() \n\n\n\n\n\nparameter\nouter_width\ninner_width\npoint_est\nll\nl\nm\nh\nhh\nmodel\n\n\n\n\nb_scaletotal_load\n0.95\n0.8\nmedian\n-0.27\n-0.25\n-0.21\n-0.17\n-0.14\nGERP\n\n\nb_scaletotal_load\n0.95\n0.8\nmedian\n-0.18\n-0.16\n-0.11\n-0.06\n-0.04\nSnpEff\n\n\n\n\n\n\n\nWe also find significant effects of both hom and het GERP and SnpEff load:\n\n\n\nHom het load results\n\n\n\n\nCode\nhomhet &lt;- read.csv(\"../output/models/intervals/hom_het_gerp45_high.csv\")\nhomhet %&gt;% kbl() \n\n\n\n\n\nparameter\nouter_width\ninner_width\npoint_est\nll\nl\nm\nh\nhh\nmodel\nloadtype\n\n\n\n\nb_scalehom_load\n0.95\n0.8\nmedian\n-0.76\n-0.70\n-0.57\n-0.45\n-0.39\nHom\nGERP\n\n\nb_scalehet_load\n0.95\n0.8\nmedian\n-0.78\n-0.72\n-0.60\n-0.48\n-0.41\nHet\nGERP\n\n\nb_scalehom_load\n0.95\n0.8\nmedian\n-0.17\n-0.15\n-0.09\n-0.04\n-0.01\nHom\nSnpEff\n\n\nb_scalehet_load\n0.95\n0.8\nmedian\n-0.24\n-0.21\n-0.15\n-0.09\n-0.06\nHet\nSnpEff\n\n\n\n\n\n\n\nHere you can find the posterior distributions of model set 1 (load on traits) for GERP and SnpEff: \n\n\n\nMS set 1 results SnpEff\n\n\nAnd for model set 2 (traits on MS) for GERP and SnpEff: \n\n\n\nMS set 2 results SnpEff\n\n\nWe can check out the result of the direct and indirect effects as follows:\n\n\nCode\neffects &lt;- read.csv(\"../output/models/annual/direct_indirect_summary.csv\")\neffects %&gt;% kbl() %&gt;%  kable_classic_2() %&gt;% scroll_box(width = \"99%\", height = \"200px\")\n\n\n\n\n\n\ntreatment\nmediator\nmethod\nindirect_median\nindirect_lower\nindirect_upper\ndirect_median\ndirect_lower\ndirect_upper\ntotal_median\ntotal_lower\ntotal_upper\npath1_median\npath1_lower\npath1_upper\npath2_median\npath2_lower\npath2_upper\nindirect_lower_80\nindirect_upper_80\ndirect_lower_80\ndirect_upper_80\n\n\n\n\nb_scaletotal_load\nb_scalelyre\ngerp45\n-0.01\n-0.04\n0.01\n-0.13\n-0.36\n0.11\n-0.14\n-0.38\n0.10\n-0.03\n-0.10\n0.04\n0.31\n-0.07\n0.69\n-0.03\n0.01\n-0.29\n0.03\n\n\nb_scaletotal_load\nb_scaleeyec\ngerp45\n0.00\n-0.02\n0.02\n-0.13\n-0.36\n0.11\n-0.13\n-0.36\n0.11\n0.00\n-0.09\n0.09\n0.13\n-0.21\n0.48\n-0.01\n0.01\n-0.29\n0.03\n\n\nb_scaletotal_load\nb_scaleblue\ngerp45\n0.00\n-0.03\n0.01\n-0.13\n-0.36\n0.11\n-0.14\n-0.38\n0.10\n-0.03\n-0.13\n0.08\n0.15\n-0.05\n0.36\n-0.02\n0.01\n-0.29\n0.03\n\n\nb_scaletotal_load\nb_scaleattend\ngerp45\n-0.13\n-0.28\n-0.01\n-0.13\n-0.36\n0.11\n-0.26\n-0.54\n0.01\n-0.10\n-0.19\n-0.01\n1.32\n0.71\n2.01\n-0.22\n-0.05\n-0.29\n0.03\n\n\nb_scaletotal_load\nb_scalefight\ngerp45\n0.00\n-0.02\n0.02\n-0.13\n-0.36\n0.11\n-0.13\n-0.36\n0.11\n0.01\n-0.09\n0.12\n-0.06\n-0.33\n0.21\n-0.01\n0.01\n-0.29\n0.03\n\n\nb_scaletotal_load\nb_scaledist\ngerp45\n-0.02\n-0.11\n0.05\n-0.13\n-0.36\n0.11\n-0.15\n-0.41\n0.11\n0.04\n-0.09\n0.16\n-0.59\n-0.93\n-0.23\n-0.07\n0.02\n-0.29\n0.03\n\n\nb_scaletotal_load\nb_scalelyre\nhigh\n0.01\n-0.01\n0.05\n-0.11\n-0.38\n0.16\n-0.09\n-0.37\n0.18\n0.04\n-0.04\n0.10\n0.33\n-0.08\n0.72\n0.00\n0.03\n-0.27\n0.06\n\n\nb_scaletotal_load\nb_scaleeyec\nhigh\n0.00\n-0.03\n0.02\n-0.11\n-0.38\n0.16\n-0.11\n-0.38\n0.16\n-0.01\n-0.10\n0.07\n0.14\n-0.21\n0.49\n-0.01\n0.01\n-0.27\n0.06\n\n\nb_scaletotal_load\nb_scaleblue\nhigh\n-0.01\n-0.04\n0.01\n-0.11\n-0.38\n0.16\n-0.12\n-0.39\n0.15\n-0.07\n-0.17\n0.04\n0.15\n-0.06\n0.36\n-0.03\n0.00\n-0.27\n0.06\n\n\nb_scaletotal_load\nb_scaleattend\nhigh\n-0.03\n-0.16\n0.09\n-0.11\n-0.38\n0.16\n-0.14\n-0.44\n0.15\n-0.03\n-0.11\n0.06\n1.32\n0.72\n1.94\n-0.11\n0.04\n-0.27\n0.06\n\n\nb_scaletotal_load\nb_scalefight\nhigh\n0.00\n-0.01\n0.02\n-0.11\n-0.38\n0.16\n-0.10\n-0.38\n0.17\n-0.02\n-0.12\n0.08\n-0.06\n-0.32\n0.20\n-0.01\n0.01\n-0.27\n0.06\n\n\nb_scaletotal_load\nb_scaledist\nhigh\n0.01\n-0.06\n0.09\n-0.11\n-0.38\n0.16\n-0.10\n-0.39\n0.17\n-0.01\n-0.14\n0.11\n-0.57\n-0.94\n-0.22\n-0.04\n0.06\n-0.27\n0.06"
  },
  {
    "objectID": "qmd/5_per_region.html#introduction",
    "href": "qmd/5_per_region.html#introduction",
    "title": "6  Load per genomic region",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nBoth functional noncoding and protein-coding regions can be under selective constraint48,49. However, the former include various regulatory elements such as promoters, enhancers and silencers, which play different roles in gene regulation and might therefore be expected to experience different strengths of purifying selection. To investigate whether the fitness effects of deleterious mutations vary by genomic region, we classified each deleterious mutation according to whether it was located within a promoter, transcription start site, intron or exon. Then, we calculate total load based on these subsets and model their effects on LMS.\n\n6.1.1 Genomic regions\nWe use a combination of GenomicRanges and rtracklayer and other packages to divide up the genome annotation file into the four genomic regions.\n\n6.1.1.1 Subset reference genome\n\n\nCode\n#### Packages #####\npacman::p_load(BiocManager, rtracklayer, GenomicFeatures, BiocGenerics, data.table, dplyr, genomation, GenomicRanges, tibble)\n\n#### Genome data ####\n# first change scaffold names\ngff_raw &lt;- fread(\"data/genomic/annotation/PO2979_Lyrurus_tetrix_black_grouse.annotation.gff\") \ngff_raw$V1 &lt;- gsub(\";\", \"__\", gff_raw$V1)\ngff_raw$V1 &lt;- gsub(\"=\", \"_\", gff_raw$V1)\nwrite.table(gff_raw, file = \"data/genomic/PO2979_Lyrurus_tetrix_black_grouse.annotation_editedscafnames.gff\", sep = \"\\t\", col.names = FALSE, quote=F, row.names = FALSE)\n\n#read in new file\ngff &lt;- makeTxDbFromGFF(\"data/genomic/PO2979_Lyrurus_tetrix_black_grouse.annotation_editedscafnames.gff\", format=\"gff3\", organism=\"Lyrurus tetrix\") \n\n## divide up between the 4 regions ###\n\npromoters &lt;- promoters(gff, upstream=2000, downstream=200, columns=c(\"tx_name\", \"gene_id\")) # From NIOO\nTSS &lt;- promoters(gff, upstream=300, downstream=50, columns=c(\"tx_name\", \"gene_id\")) # TSS as in Laine et al., 2016. Nature Communications\nexons_gene &lt;- unlist(exonsBy(gff, \"gene\")) # group exons by genes\nintrons &lt;- unlist(intronsByTranscript(gff, use.names=TRUE))\n\n### write out files\nexport(promoters, \"data/genomic/annotation/promoters.gff3\")\nexport(TSS, \"data/genomic/annotation/TSS.gff3\")\nintrons@ranges@NAMES[is.na(introns@ranges@NAMES)]&lt;-\"Unknown\"\nexport(introns, \"data/genomic/annotation/introns_transcripts.gff3\")\nexport(exons_gene, \"data/genomic/annotation/exons_gene.gff3\")\n\n\n\n\n6.1.1.2 Annotate deleterious mutations\nThen, we load in all deleterious mutations for GERP and SnpEff separately and annotate the mutations according to the four regions.\n\n\nCode\n###### Annotate high impact and GERP mutations #####\n### annotate gene regions per SNP for high impact and gerp mutations ###\n\n### load data ###\n\nload(file = \"output/load/snpeff/snpeff_high.RData\")\nload(file = \"output/load/gerp/gerp_over4.RData\")\n\n### change scaf names\nsnpeff$CHROM &lt;- gsub(\";\", \"__\", snpeff$CHROM)\nsnpeff$CHROM &lt;- gsub(\"=\", \"_\", snpeff$CHROM)\n\ngerp$chr &lt;- gsub(\";\", \"__\", gerp$chr)\ngerp$chr &lt;- gsub(\"=\", \"_\", gerp$chr)\n\n### remove genotypes: not necessary here\nsnpeff &lt;- snpeff[,c(1:9)]\ngerp &lt;- gerp[,c(1:9)]\n\n####### Gene regions ###########\n\n### load annotation data\nannotation_dir &lt;- \"data/genomic/annotation\"\n\npromoter=unique(gffToGRanges(paste0(annotation_dir, \"/promoters.gff3\")))\nTSS=unique(gffToGRanges(paste0(annotation_dir, \"/TSS.gff3\")))\nintrons=unique(gffToGRanges(paste0(annotation_dir, \"/introns_transcripts.gff3\")))\nexons_gene=unique(gffToGRanges(paste0(annotation_dir, \"/exons_gene.gff3\")))\n\n#### Annotate SNPeff regions ####\nsnpeff$end &lt;- snpeff$POS\nsnpeff$start &lt;- snpeff$POS\nsnpef_gr &lt;- as(snpeff, \"GRanges\")\n\nsnpef_promoter &lt;- as.data.frame(subsetByOverlaps(snpef_gr, promoter)) %&gt;%\n  add_column(\"region_promoter\" = 1) %&gt;% dplyr::rename(chr = seqnames)%&gt;% unique()\n\nsnpef_TSS &lt;- as.data.frame(subsetByOverlaps(snpef_gr, TSS))%&gt;%\n  add_column(\"region_tss\" = 1) %&gt;% dplyr::rename(chr = seqnames)%&gt;% unique()\n\nsnpef_exons &lt;- as.data.frame(subsetByOverlaps(snpef_gr, exons_gene))%&gt;%\n  add_column(\"region_exon\" = 1) %&gt;% dplyr::rename(chr = seqnames)%&gt;% unique()\n\nsnpef_introns &lt;- as.data.frame(subsetByOverlaps(snpef_gr, introns))%&gt;%\n  add_column(\"region_intron\" = 1) %&gt;% dplyr::rename(chr = seqnames)%&gt;% unique()\n\nsnpef_all &lt;- left_join(snpeff, snpef_promoter[,c(\"chr\", \"start\", \"region_promoter\")], by = c(\"CHROM\" = \"chr\", \"start\"))%&gt;%\n  left_join(snpef_TSS[,c(\"chr\", \"start\", \"region_tss\")], by = c(\"CHROM\" = \"chr\", \"start\"))%&gt;%\n  left_join(snpef_exons[,c(\"chr\", \"start\", \"region_exon\")], by = c(\"CHROM\" = \"chr\", \"start\"))%&gt;%\n  left_join(snpef_introns[,c(\"chr\", \"start\", \"region_intron\")], by = c(\"CHROM\" = \"chr\", \"start\"))\n\n# correct promoters\nsnpef_all &lt;- snpef_all %&gt;% mutate(region_promoter = case_when(\n  region_tss == 1 & region_promoter == 1 ~ NA,\n  is.na(region_tss) & region_promoter == 1 ~ 1\n))\n\nsave(snpef_all, file = \"output/load/snpeff/snpeff_high_annotated_region.RData\")\n\n#### Annotate gerp regions ####\ngerp$end &lt;- gerp$pos\ngerp$start &lt;- gerp$pos\ngerp_gr &lt;- as(gerp, \"GRanges\")\n\ngerp_promoter &lt;- as.data.frame(subsetByOverlaps(gerp_gr, promoter)) %&gt;%\n  add_column(\"region_promoter\" = 1) %&gt;% dplyr::rename(chr = seqnames)%&gt;% unique()\n\ngerp_TSS &lt;- as.data.frame(subsetByOverlaps(gerp_gr, TSS))%&gt;%\n  add_column(\"region_tss\" = 1) %&gt;% dplyr::rename(chr = seqnames)%&gt;% unique()\n\ngerp_exons &lt;- as.data.frame(subsetByOverlaps(gerp_gr, exons_gene))%&gt;%\n  add_column(\"region_exon\" = 1) %&gt;% dplyr::rename(chr = seqnames)%&gt;% unique()\n\ngerp_introns &lt;- as.data.frame(subsetByOverlaps(gerp_gr, introns))%&gt;%\n  add_column(\"region_intron\" = 1) %&gt;% dplyr::rename(chr = seqnames)%&gt;% unique()\n\ngerp_all &lt;- left_join(gerp, gerp_promoter[,c(\"chr\", \"start\", \"region_promoter\")], by = c(\"chr\" = \"chr\", \"start\"))%&gt;%\n   left_join(gerp_TSS[,c(\"chr\", \"start\", \"region_tss\")], by = c(\"chr\" = \"chr\", \"start\"))%&gt;%\n  left_join(gerp_exons[,c(\"chr\", \"start\", \"region_exon\")], by = c(\"chr\" = \"chr\", \"start\"))%&gt;%\n  left_join(gerp_introns[,c(\"chr\", \"start\", \"region_intron\")], by = c(\"chr\" = \"chr\", \"start\"))\n \n# correct promoters\ngerp_all &lt;- gerp_all %&gt;% mutate(region_promoter = case_when(\n  region_tss == 1 & region_promoter == 1 ~ NA,\n  is.na(region_tss) & region_promoter == 1 ~ 1\n))\n\nsave(gerp_all, file = \"output/load/gerp/gerp_annotated_region.RData\")\n\n\n\n\n6.1.1.3 Calculate mutation load\nThese two files (gerp_all and snpeff_all) contain the SNP locations and additional binary columns whether the mutations was located in one of the 4 regions. We can use this file together with the files containing the genotypes to calculate mutation load based on the subsets of mutations only.\n\n\nCode\n#### Calculate load per region #####\n\n# load annotated gerp data\nload(file = \"output/load/gerp/gerp_annotated_region.RData\")\ngerp_all$chr &lt;- gsub(\"__\", \";\", gerp_all$chr)\ngerp_all$chr &lt;- gsub(\"HRSCAF_\", \"HRSCAF=\", gerp_all$chr)\n\n# load annotation\nload(file = \"output/load/snpeff/snpeff_high_annotated_region.RData\")\nsnpef_all$CHROM &lt;- gsub(\"__\", \";\", snpef_all$CHROM)\nsnpef_all$CHROM &lt;- gsub(\"HRSCAF_\", \"HRSCAF=\", snpef_all$CHROM)\n\n## load functions to calculate load\nsource(\"scripts/7_calculate_load/function_calculate_load.R\")\n\n#### calculate load per region as defined below \n\nregions &lt;- c(\"region_promoter\", \"region_tss\" ,\"region_exon\",\"region_intron\")\n\n#load existing combined load file\nload(\"output/load/all_loads_combined_da_nosex_29scaf.RData\") #loads no sex chr only 29 scaf\n\n# load gt again to include genotypes\nload(file = \"output/load/snpeff/snpeff_high.RData\")\nload(file = \"output/load/gerp/gerp_over4.RData\")\n\n#first gerp5\nload_per_region &lt;- load\nfor (region in regions){\n  subset_locs &lt;- subset(gerp_all, gerp_all[,region] == 1) #subset based on region name\n  \n  subset_locs$chr_pos &lt;- paste0(subset_locs$chr, \"_\", subset_locs$pos) #make a col for the snp position\n  gerp$chr_pos &lt;- paste0(gerp$chr, \"_\", gerp$pos) #make a col for the snp position\n  \n  sub_genotypes &lt;- subset(gerp, chr_pos %in% subset_locs$chr_pos) #subset genotypes based on subset\n  \n  sub_genotypes$chr_pos &lt;- NULL #remove chr_pos again\n  \n  loadtype = paste0(\"gerp45\", gsub(\"region\", \"\", region))\n  \n  load_sub &lt;- calculate_load_gerp(sub_genotypes, loadtype = loadtype, output_vcf = F) #calculate loads\n  \n  load_per_region &lt;- rbind(load_per_region, load_sub[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")])\n}\n\n#snpeff\nfor (region in regions){\n  subset_locs &lt;- subset(snpef_all, snpef_all[,region] == 1) #subset based on region name\n  \n  subset_locs$chr_pos &lt;- paste0(subset_locs$CHROM, \"_\", subset_locs$POS) #make a col for the snp position\n  snpeff$chr_pos &lt;- paste0(snpeff$CHROM, \"_\", snpeff$POS) #make a col for the snp position\n  \n  sub_genotypes &lt;- subset(snpeff, chr_pos %in% subset_locs$chr_pos) #subset genotypes based on subset\n  \n  sub_genotypes$chr_pos &lt;- NULL #remove chr_pos again\n  \n  loadtype = paste0(\"high\", gsub(\"region\", \"\", region))\n  load_sub &lt;- calculate_load_snpeff(sub_genotypes, loadtype = loadtype, output_vcf = F) #calculate loads\n  \n  load_per_region &lt;- rbind(load_per_region, load_sub[,c(\"id\", \"het_load\", \"hom_load\", \"total_load\", \"loadtype\")])\n}\n\nsave(load_per_region, file = \"output/load/all_loads_combined_da_nosex_29scaf_plus_per_region.RData\")\n\n\n\n\n\n6.1.2 Modelling\nWe then computed the exact same models as the total load models presented before with the following model structure:\nLMS ~ scale(total_load) + core + (1|site)\nWhere total load is not a measure taken from a subset of deleterious mutations located within a specific genomic region.\n\n\n6.1.3 Results\nBelow you find the posterior distributions:\n\n\n\nResults per genomic region GERP\n\n\n\n\n\nResults per genomic region SnpEff\n\n\n\n\nCode\nlibrary(readxl); library(dplyr); library(kableExtra)\ngerp &lt;- read.csv(\"../output/models/intervals/regions_gerp45.csv\")\ngerp %&gt;% kbl() \n\n\n\n\n\nparameter\nouter_width\ninner_width\npoint_est\nll\nl\nm\nh\nhh\nregion\n\n\n\n\nb_scaletotal_load\n0.95\n0.8\nmedian\n-0.26\n-0.23\n-0.18\n-0.13\n-0.09\nPromoter\n\n\nb_scaletotal_load\n0.95\n0.8\nmedian\n-0.35\n-0.32\n-0.27\n-0.22\n-0.20\nTSS\n\n\nb_scaletotal_load\n0.95\n0.8\nmedian\n-0.37\n-0.35\n-0.29\n-0.23\n-0.21\nIntron\n\n\nb_scaletotal_load\n0.95\n0.8\nmedian\n0.15\n0.18\n0.23\n0.29\n0.31\nExon\n\n\n\n\n\n\n\nCode\nhigh &lt;- read.csv(\"../output/models/intervals/regions_high.csv\")\nhigh %&gt;% kbl()\n\n\n\n\n\nparameter\nouter_width\ninner_width\npoint_est\nll\nl\nm\nh\nhh\nregion\n\n\n\n\nb_scaletotal_load\n0.95\n0.8\nmedian\n-0.34\n-0.31\n-0.26\n-0.20\n-0.18\nPromoter\n\n\nb_scaletotal_load\n0.95\n0.8\nmedian\n-0.12\n-0.09\n-0.04\n0.01\n0.04\nTSS\n\n\nb_scaletotal_load\n0.95\n0.8\nmedian\n-0.15\n-0.13\n-0.08\n-0.02\n0.01\nIntron\n\n\nb_scaletotal_load\n0.95\n0.8\nmedian\n-0.20\n-0.17\n-0.11\n-0.06\n-0.03\nExon\n\n\n\n\n\n\n\nThese results indicate that regulatory regions, especially promoter regions (SnpEff) are very important!"
  },
  {
    "objectID": "qmd/6_random_sub.html#introduction",
    "href": "qmd/6_random_sub.html#introduction",
    "title": "7  Random subsampling",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nVariation in the effects of mutation load among the various models presented could be attributed to variation in the effect sizes of the individual mutations, or alternatively due to the differences in the number of mutations that contribute to the mutation load estimate. Assuming that all mutations that are identified have negative fitness consequences to some extent, we can expect that a larger number of mutations explain more fitness variation. We tested for the effect of total load on LMS between GERP and SnpEff mutations and the four genomic regions by controlling for the number of mutations."
  },
  {
    "objectID": "qmd/6_random_sub.html#methods",
    "href": "qmd/6_random_sub.html#methods",
    "title": "7  Random subsampling",
    "section": "7.2 Methods",
    "text": "7.2 Methods\nWe randomly subset over all deleterioius GERP and SnpEff mutations (separately), and per genomic region for the two approaches.\nFirst, we load in our data and make different subsets of the data: all GERP mutations, GERP mutations per region (4 dataframes) and the same for SnpEff mutations\n\n\nCode\n### load packages\npacman::p_load(tidyverse, data.table)\n\n#load data mutations\nload(file = \"output/load/snpeff/snpeff_high.RData\")\nload(file = \"output/load/gerp/gerp_over4.RData\")\n\n# load annotation data\nload(file = \"output/load/snpeff/snpeff_high_annotated_region.RData\")\nsnpef_all$CHROM &lt;- gsub(\"__\", \";\", snpef_all$CHROM)\nsnpef_all$CHROM &lt;- gsub(\"HRSCAF_\", \"HRSCAF=\", snpef_all$CHROM)\nsnpef_all$chr_pos &lt;- paste0(snpef_all$CHROM, \"_\", snpef_all$POS)\n\nload(file = \"output/load/gerp/gerp_annotated_region.RData\")\ngerp_all$chr &lt;- gsub(\"__\", \";\", gerp_all$chr)\ngerp_all$chr &lt;- gsub(\"HRSCAF_\", \"HRSCAF=\", gerp_all$chr)\ngerp_all$chr_pos &lt;- paste0(gerp_all$chr, \"_\", gerp_all$pos)\n\n### make subsets of mutations based on region\n# subset snpef based on annotation\nsnpeff_exons &lt;- subset(snpef_all, region_exon == 1)\nsnpeff_tss &lt;- subset(snpef_all, region_tss == 1)\nsnpeff_introns &lt;- subset(snpef_all, region_intron == 1)\nsnpeff_promoter &lt;- subset(snpef_all, region_promoter == 1 & is.na(region_tss))\n\nsnpeff$chr_pos &lt;- paste0(snpeff$CHROM, \"_\", snpeff$POS)\n\nsnpeff_exons_gt &lt;- subset(snpeff, chr_pos %in% snpeff_exons$chr_pos)\nsnpeff_tss_gt &lt;- subset(snpeff, chr_pos %in% snpeff_tss$chr_pos)\nsnpeff_introns_gt &lt;- subset(snpeff, chr_pos %in% snpeff_introns$chr_pos)\nsnpeff_promoter_gt &lt;- subset(snpeff, chr_pos %in% snpeff_promoter$chr_pos)\n\n# subset gerp based on annotation\ngerp_exons &lt;- subset(gerp_all, region_exon == 1)\ngerp_tss &lt;- subset(gerp_all, region_tss == 1)\ngerp_introns &lt;- subset(gerp_all, region_intron == 1)\ngerp_promoter &lt;- subset(gerp_all, region_promoter == 1 & is.na(region_tss))\n\ngerp$chr_pos &lt;- paste0(gerp$chr, \"_\", gerp$pos)\n\ngerp_exons_gt &lt;- subset(gerp, chr_pos %in% gerp_exons$chr_pos)\ngerp_tss_gt &lt;- subset(gerp, chr_pos %in% gerp_tss$chr_pos)\ngerp_introns_gt &lt;- subset(gerp, chr_pos %in% gerp_introns$chr_pos)\ngerp_promoter_gt &lt;- subset(gerp, chr_pos %in% gerp_promoter$chr_pos)\n\n\nWe then create a function to execute the subsetting:\n\n\nCode\n## create function to subset X number of SnpEffs and model its effect on fitness\n\nrandom_draws &lt;- function(geno, n_draws, n_mutations, file, method, emperical_beta){\n  source(\"scripts/theme_ggplot.R\")\n  all_draws &lt;- data.frame()\n  \n  if(method == \"GERP\"){\n    for (i in 1:n_draws){\n      draw &lt;- geno[sample(nrow(geno), n_mutations),] #randomly draw snps\n      ## load functions\n      source(\"scripts/7_calculate_load/function_calculate_load.R\")\n      load &lt;- calculate_load_gerp(draw, output_vcf = F, loadtype = \"random_draw\") #calculate load\n      model_out &lt;- model_load(load, i)\n      model_out$method &lt;- method\n      \n      all_draws &lt;- rbind(all_draws, model_out)\n    }}\n  \n  if(method == \"High impact\"){\n    for (i in 1:n_draws){\n      draw &lt;- geno[sample(nrow(geno), n_mutations),] #randomly draw snps\n      ## load functions\n      source(\"scripts/7_calculate_load/function_calculate_load.R\")\n      load &lt;- calculate_load_snpeff(draw, output_vcf = F, loadtype = \"random_draw\") #calculate load\n      model_out &lt;- model_load(load, i)\n      model_out$method &lt;- method\n      \n      all_draws &lt;- rbind(all_draws, model_out)\n    }}\n  \n  ### conclusion\n  all_draws &lt;- all_draws %&gt;% mutate(conclusion = as.factor(case_when(\n    beta &lt; 0 & pval &lt; 0.05 ~ \"Significantly negative\",\n    beta &gt; 0 & pval &lt; 0.05 ~ \"Significantly positive\",\n    TRUE ~ \"Insignificant\"\n  )))\n  \n  return(all_draws)\n  save(all_draws, file = file)\n}\n\n### the model_load function looks like follows:\n\n\nmodel_load &lt;- function(load, ndraw){\n  #join with phenotypes\n  load(\"data/phenotypes/phenotypes_lifetime.RData\") #LMS\n  pheno &lt;- pheno_wide %&gt;% mutate(core = as.factor(case_when(is.na(LMS) ~ \"no core\", !is.na(LMS) ~ \"core\")))\n  \n  data_pheno &lt;- left_join(load, pheno[,c(\"id\", \"LMS\", \"LMS_min\", \"core\", \"site\", \"born\")], by = \"id\")\n  \n  model &lt;- glmmTMB::glmmTMB(LMS_min ~ scale(total_load) + core + (1|site), family = \"poisson\", ziformula = ~1, data = data_pheno)\n  summary &lt;- summary(model)\n  \n  beta &lt;- summary$coefficients$cond[\"scale(total_load)\",\"Estimate\"]\n  se &lt;- summary$coefficients$cond[\"scale(total_load)\",\"Std. Error\"]\n  zval &lt;- summary$coefficients$cond[\"scale(total_load)\",\"z value\"]\n  pval &lt;- summary$coefficients$cond[\"scale(total_load)\",\"Pr(&gt;|z|)\"]\n  \n  out &lt;- data.frame(ndraw = ndraw, \n                    beta = beta,\n                    se = se,\n                    zval = zval,\n                    pval = pval)\n  \n  return(out)\n}\n\n\nThen we run this function for the 10 subsets of mutations, with varying number of mutations that get sampled:\n\n\nCode\n# run functions\n\n# all mutations\ndraws_gerp &lt;- random_draws(geno = gerp, n_draws = 5000, n_mutations = 1000, file = \"output/random_draws/all_gerp.RData\", method=\"GERP\", emperical_beta = -0.21)\nsave(draws_gerp, file = \"output/random_draws/all_gerp.RData\")\n\ndraws_high &lt;- random_draws(geno = snpeff, n_draws = 5000, n_mutations = 1000, file = \"output/random_draws/all_high.RData\", method = \"High impact\", emperical_beta = -0.07)\nsave(draws_high, file = \"output/random_draws/all_high.RData\")\n\n# per region\n# high\ndraws_high_promoter &lt;- random_draws(geno = snpeff_promoter_gt, n_draws = 5000, n_mutations = 500, file = \"results/random_draws/promoter_high.RData\", method = \"High impact\", emperical_beta = 0)\nsave(draws_high_promoter, file = \"output/random_draws/promoter_high.RData\")\n\ndraws_high_tss &lt;- random_draws(geno = snpeff_tss_gt, n_draws = 5000, n_mutations = 500, file = \"results/random_draws/tss_high.RData\", method = \"High impact\", emperical_beta = 0)\nsave(draws_high_tss, file = \"output/random_draws/tss_high.RData\")\n\ndraws_high_intron &lt;- random_draws(geno = snpeff_introns_gt, n_draws = 5000, n_mutations = 500, file = \"results/random_draws/intron_high.RData\", method = \"High impact\", emperical_beta = 0)\nsave(draws_high_intron, file = \"output/random_draws/intron_high.RData\")\n\ndraws_high_exon &lt;- random_draws(geno = snpeff_exons_gt, n_draws = 5000, n_mutations = 500, file = \"results/random_draws/exon_high.RData\", method = \"High impact\", emperical_beta = 0)\nsave(draws_high_exon, file = \"output/random_draws/exon_high.RData\")\n\n# gerp\ndraws_gerp_promoter &lt;- random_draws(geno = gerp_promoter_gt, n_draws = 5000, n_mutations = 500, file = \"results/random_draws/promoter_gerp.RData\", method = \"GERP\", emperical_beta = 0)\nsave(draws_gerp_promoter, file = \"output/random_draws/promoter_gerp.RData\")\n\ndraws_gerp_tss &lt;- random_draws(geno = gerp_tss_gt, n_draws = 5000, n_mutations = 500, file = \"results/random_draws/tss_gerp.RData\", method = \"GERP\", emperical_beta = 0)\nsave(draws_gerp_tss, file = \"output/random_draws/tss_gerp.RData\")\n\ndraws_gerp_intron &lt;- random_draws(geno = gerp_introns_gt, n_draws = 5000, n_mutations = 500, file = \"results/random_draws/intron_gerp.RData\", method = \"GERP\", emperical_beta = 0)\nsave(draws_gerp_intron, file = \"output/random_draws/intron_gerp.RData\")\n\ndraws_gerp_exon &lt;- random_draws(geno = gerp_exons_gt, n_draws = 5000, n_mutations = 500, file = \"results/random_draws/exon_gerp.RData\", method = \"GERP\", emperical_beta = 0)\nsave(draws_gerp_exon, file = \"output/random_draws/exon_gerp.RData\")"
  },
  {
    "objectID": "qmd/6_random_sub.html#results",
    "href": "qmd/6_random_sub.html#results",
    "title": "7  Random subsampling",
    "section": "7.3 Results",
    "text": "7.3 Results\nWe can then plot the results as histograms:\n\n\nCode\n### load packages ####\npacman::p_load(tidyverse, ggpubr, extrafont, cowplot, data.table)\n\nsource(\"scripts/theme_ggplot.R\")\n\n#### total GERP #####\nload(file=\"output/random_draws/all_gerp.RData\")\nload(file=\"output/random_draws/all_high.RData\")\n\nsum_gerp &lt;- draws_gerp %&gt;%\n  summarize(lower_95 = quantile(beta, probs=c(0.025)),\n            upper_95 = quantile(beta, probs=c(0.975)),\n            lower_80 = quantile(beta, probs=c(0.1)),\n            upper_80 = quantile(beta, probs=c(0.9)),\n            mean = mean(beta))\n\nggplot(draws_gerp, aes(x = beta)) + \n  xlim(-1.2,1.2)+\n  ylim(-50, 600)+\n  geom_histogram(aes(fill = beta &lt; 0, col = beta &lt; 0), linewidth=0.5, bins=40)+\n  scale_fill_manual(values =alpha(c(\"grey60\", clr_gerp), 0.7)) + #\n  scale_color_manual(values =c(\"grey60\", clr_gerp)) +\n  geom_segment(data=sum_gerp, aes(x = lower_95, \n                             xend = upper_95, \n                             y = 0), col = \"black\", linewidth=1)+\n  geom_segment(data=sum_gerp, aes(x = lower_80, \n                             xend = upper_80, \n                             y = 0), col = \"black\", linewidth=3)+\n  geom_point(data=sum_gerp,aes(x = mean, y = 0), fill=\"white\",  col = \"black\", shape=21, size = 6)+\n  geom_vline(xintercept = 0, col = \"#ca562c\", linetype=\"longdash\", linewidth=0.6)+\n  theme(plot.title = element_text(margin=margin(0,0,30,0)),\n        panel.border = element_blank(),\n        panel.grid = element_blank(),\n        panel.spacing = unit(3,\"lines\"),\n        strip.background = element_blank(),\n        legend.position=\"none\")+\n  labs(x = expression(\"Standardised\"~beta), y = \"# draws\", title = \"GERP\") -&gt; total_gerp\n\n\nWe can then repeat this for the other subsets, leading to the following plots:\n\n\n\nRandom subsets"
  },
  {
    "objectID": "qmd/references.html",
    "href": "qmd/references.html",
    "title": "References",
    "section": "",
    "text": "Bertorelle, Giorgio, Francesca Raffini, Mirte Bosse, Chiara Bortoluzzi,\nAlessio Iannucci, Emiliano Trucchi, Hernán E. Morales, and Cock van\nOosterhout. 2022. “Genetic Load: Genomic Estimates and\nApplications in Non-Model Animals.” Nature Reviews\nGenetics 23 (8): 492–503. https://doi.org/10.1038/s41576-022-00448-x.\n\n\nCingolani, Pablo, Adrian Platts, Le Lily Wang, Melissa Coon, Tung\nNguyen, Luan Wang, Susan J. Land, Xiangyi Lu, and Douglas M. Ruden.\n2012. “A Program for Annotating and Predicting the Effects of\nSingle Nucleotide Polymorphisms, SnpEff.”\nFly 6 (2): 80–92. https://doi.org/10.4161/fly.19695.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper,\nArend Sidow, and Serafim Batzoglou. 2010. “Identifying a\nHigh Fraction of the Human Genome to Be Under\nSelective Constraint Using GERP++.” Edited by Wyeth\nW. Wasserman. PLoS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025."
  }
]